{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "uCZbvutfCmOL",
    "outputId": "bf211bc4-fc98-4d2d-a8f9-84a381ff627a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__pycache__\n",
      "assets\n",
      "getCppFiles.py\n",
      "lstm_generate_cpp_code.ipynb\n",
      "randomSequence.py\n",
      "readme.md\n",
      "splitCppData.py\n",
      "tempCodeRunnerFile.py\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from randomSequence import getTrainLoader\n",
    "train_loader = g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TRg8lCu1CmPn",
    "outputId": "4bf84cfe-8c21-48aa-8c62-9eb48c01be56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the PyTorch RNN Module\n",
    "        :param vocab_size: The number of input dimensions of the neural network (the size of the vocabulary)\n",
    "        :param output_size: The number of output dimensions of the neural network\n",
    "        :param embedding_dim: The size of embeddings, should you choose to use them        \n",
    "        :param hidden_dim: The size of the hidden layer outputs\n",
    "        :param dropout: dropout to add in between LSTM/GRU layers\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "        # TODO: Implement function\n",
    "        \n",
    "        # set class variables\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # define model layers\n",
    "        self.embedding = nn.Embedding(vocab_size + 1, embedding_dim)\n",
    "        # 之前是vocab_size而不是vocab_size + 1. 一直报错: RuntimeError: index out of range at /opt/conda/conda-bld/pytorch_1524584710464/work/aten/src/TH/gene.\n",
    "        # 参考: https://blog.csdn.net/Geek_of_CSDN/article/details/86527107\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                           dropout=dropout, batch_first=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "    \n",
    "    \n",
    "    def forward(self, nn_input, hidden):\n",
    "        \"\"\"\n",
    "        Forward propagation of the neural network\n",
    "        :param nn_input: The input to the neural network\n",
    "        :param hidden: The hidden state        \n",
    "        :return: Two Tensors, the output of the neural network and the latest hidden state\n",
    "        \"\"\"\n",
    "        # TODO: Implement function   \n",
    "        x = nn_input\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        \n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # sig_out = self.sig(out)\n",
    "        # 根据交流群同学的提示, 这里要取消sigmoid. 记得之前做卷积神经网络项目的时候也出现过这个问题\n",
    "        sig_out = out\n",
    "        \n",
    "        sig_out = sig_out.view(batch_size, -1, self.output_size)\n",
    "        sig_out = sig_out[:, -1]\n",
    "        \n",
    "        # return one batch of output word scores and the hidden state\n",
    "        return sig_out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        '''\n",
    "        Initialize the hidden state of an LSTM/GRU\n",
    "        :param batch_size: The batch_size of the hidden state\n",
    "        :return: hidden state of dims (n_layers, batch_size, hidden_dim)\n",
    "        '''\n",
    "        # Implement function\n",
    "        \n",
    "        # initialize hidden state with zero weights, and move to GPU if available\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if train_on_gpu:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                     weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_rnn(RNN, train_on_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dJK5vTWKCmPq"
   },
   "source": [
    "### 定义前向及后向传播\n",
    "\n",
    "通过你实现的 RNN 类来进行前向及后项传播。你可以在训练循环中，不断地调用如下代码来实现：\n",
    "```\n",
    "loss, hidden = forward_back_prop(decoder, decoder_optimizer, criterion, inp, target)\n",
    "```\n",
    "\n",
    "函数中需要返回一个批次以及其隐藏状态的loss均值，你可以调用一个函数`RNN(inp, hidden)`来实现。记得，你可以通过调用`loss.item()` 来计算得到该loss。\n",
    "\n",
    "**如果使用 GPU，你需要将你的数据存到 GPU 的设备上。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hx9r-9hGCmPq",
    "outputId": "a406518e-f088-4955-f7a4-0e2e1310574d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def forward_back_prop(rnn, optimizer, criterion, inp, target, hidden):\n",
    "    \"\"\"\n",
    "    Forward and backward propagation on the neural network\n",
    "    :param decoder: The PyTorch Module that holds the neural network\n",
    "    :param decoder_optimizer: The PyTorch optimizer for the neural network\n",
    "    :param criterion: The PyTorch loss function\n",
    "    :param inp: A batch of input to the neural network\n",
    "    :param target: The target output for the batch of input\n",
    "    :return: The loss and the latest hidden state Tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    # move data to GPU, if available\n",
    "    if train_on_gpu:\n",
    "        inp, target = inp.cuda(), target.cuda()\n",
    "    \n",
    "    # perform backpropagation and optimization\n",
    "    rnn.zero_grad()\n",
    "    # 修改\n",
    "    hidden = tuple([each.data for each in hidden])\n",
    "    output, hidden = rnn(inp, hidden)\n",
    "    # 修改\n",
    "    loss = criterion(output, target.long())\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    # return the loss over a batch and the hidden state produced by our model\n",
    "    return loss.item(), hidden\n",
    "\n",
    "# Note that these tests aren't completely extensive.\n",
    "# they are here to act as general checks on the expected outputs of your functions\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_forward_back_prop(RNN, forward_back_prop, train_on_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AgBJRweXCmPs"
   },
   "source": [
    "## 神经网络训练\n",
    "\n",
    "神经网络结构完成以及数据准备完后，我们可以开始训练网络了。\n",
    "\n",
    "### 训练循环\n",
    "\n",
    "训练循环是通过 `train_decoder` 函数实现的。该函数将进行 epochs 次数的训练。模型的训练成果会在一定批次的训练后，被打印出来。这个“一定批次”可以通过`show_every_n_batches` 来设置。你会在下一节设置这个参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mZvsfb1jCmPt"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "\n",
    "def train_rnn(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches=100):\n",
    "    batch_losses = []\n",
    "    \n",
    "    rnn.train()\n",
    "\n",
    "    print(\"Training for %d epoch(s)...\" % n_epochs)\n",
    "    for epoch_i in range(1, n_epochs + 1):\n",
    "        \n",
    "        # initialize hidden state\n",
    "        hidden = rnn.init_hidden(batch_size)\n",
    "        \n",
    "        for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n",
    "            \n",
    "            # make sure you iterate over completely full batches, only\n",
    "            n_batches = len(train_loader.dataset)//batch_size\n",
    "            if(batch_i > n_batches):\n",
    "                break\n",
    "            \n",
    "            # forward, back prop\n",
    "            loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden)          \n",
    "            # record loss\n",
    "            batch_losses.append(loss)\n",
    "\n",
    "            # printing loss stats\n",
    "            if batch_i % show_every_n_batches == 0:\n",
    "                print('Epoch: {:>4}/{:<4}  Loss: {}\\n'.format(\n",
    "                    epoch_i, n_epochs, np.average(batch_losses)))\n",
    "                batch_losses = []\n",
    "\n",
    "    # returns a trained rnn\n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KbaaWMpqCmPv"
   },
   "source": [
    "### 超参数\n",
    "\n",
    "设置并训练以下超参数:\n",
    "-  `sequence_length`，序列长度 \n",
    "-  `batch_size`，分批大小\n",
    "-  `num_epochs`，循环次数\n",
    "-  `learning_rate`，Adam优化器的学习率\n",
    "-  `vocab_size`，唯一标示词汇的数量\n",
    "-  `output_size`，模型输出的大小 \n",
    "-  `embedding_dim`，词嵌入的维度，小于 vocab_size\n",
    "-  `hidden_dim`， 隐藏层维度\n",
    "-  `n_layers`， RNN的层数\n",
    "-  `show_every_n_batches`，打印结果的频次\n",
    "\n",
    "如果模型没有获得你预期的结果，调整 `RNN`类中的上述参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "6MtYhJhDCmPw",
    "outputId": "d3dd14d3-72cf-4198-d1f5-c40a55ee481b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "892090\n",
      "892090\n"
     ]
    }
   ],
   "source": [
    "# Data params\n",
    "# Sequence Length\n",
    "sequence_length = 20  # of words in a sequence\n",
    "# Batch Size\n",
    "batch_size = 256\n",
    "\n",
    "# data loader - do not change\n",
    "train_loader = batch_data(int_text, sequence_length, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "R8qRmw6WCmPz",
    "outputId": "5629c79e-ef15-422f-c6f9-2fae3ee7a720"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21386\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(max(int_text))\n",
    "print(min(int_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yz2HzHhpCmP2"
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "# Number of Epochs\n",
    "num_epochs = 7\n",
    "# Learning Rate\n",
    "learning_rate = 0.0018\n",
    "\n",
    "# Model parameters\n",
    "# Vocab size\n",
    "vocab_size = max(int_text) + 1  # 词的数量, 从0开始索引, 所以需要+1\n",
    "# Output size\n",
    "output_size = vocab_size\n",
    "# Embedding Dimension\n",
    "embedding_dim = 256\n",
    "# Hidden Dimension\n",
    "hidden_dim = 512\n",
    "# Number of RNN Layers\n",
    "n_layers = 2\n",
    "\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E41Kb01uCmP6"
   },
   "source": [
    "### 训练\n",
    "下一节，通过预处理数据来训练神经网络。如果你的loss结果不好，可以通过调整超参数来修正。通常情况下，大的隐藏层及层数会带来比较好的效果，但同时也会消耗较长的时间来训练。\n",
    "> **你应该努力得到一个低于3.5的loss** \n",
    "\n",
    "你也可以试试不同的序列长度，该参数表明模型学习的范围大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R3UaTuwMCmP7",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run():\n",
    "    \"\"\"\n",
    "    DON'T MODIFY ANYTHING IN THIS CELL\n",
    "    \"\"\"\n",
    "\n",
    "    # create model and move to gpu if available\n",
    "    rnn = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.0)\n",
    "    if train_on_gpu:\n",
    "        rnn.cuda()\n",
    "\n",
    "    # defining loss and optimization functions for training\n",
    "    optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # training the model\n",
    "    trained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, show_every_n_batches)\n",
    "\n",
    "    # saving the trained model\n",
    "    helper.save_model('./save/trained_rnn', trained_rnn)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "sObhzxnhCmP_",
    "outputId": "8eb1c71c-7a80-4c0b-e043-e1c082f269df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data  helper.py  preprocess.p  problem_unittests.py  __pycache__  sample_data\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "1GFGgEhRo0Vg",
    "outputId": "a872f96c-1b01-4a2f-80f9-dda8b3cfdb79",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 7 epoch(s)...\n",
      "Epoch:    1/7     Loss: 6.294025840759278\n",
      "\n",
      "Epoch:    1/7     Loss: 5.569184093475342\n",
      "\n",
      "Epoch:    1/7     Loss: 5.164338521957397\n",
      "\n",
      "Epoch:    1/7     Loss: 4.932125067710876\n",
      "\n",
      "Epoch:    1/7     Loss: 4.806701436042785\n",
      "\n",
      "Epoch:    1/7     Loss: 4.725909733772278\n",
      "\n",
      "Epoch:    1/7     Loss: 4.620312347412109\n",
      "\n",
      "Epoch:    1/7     Loss: 4.604720420837403\n",
      "\n",
      "Epoch:    1/7     Loss: 4.52324640750885\n",
      "\n",
      "Epoch:    1/7     Loss: 4.508321452140808\n",
      "\n",
      "Epoch:    1/7     Loss: 4.496301078796387\n",
      "\n",
      "Epoch:    1/7     Loss: 4.4042434930801395\n",
      "\n",
      "Epoch:    1/7     Loss: 4.391848502159118\n",
      "\n",
      "Epoch:    1/7     Loss: 4.376086373329162\n",
      "\n",
      "Epoch:    1/7     Loss: 4.3514355945587155\n",
      "\n",
      "Epoch:    1/7     Loss: 4.324425203800201\n",
      "\n",
      "Epoch:    1/7     Loss: 4.295239098072052\n",
      "\n",
      "Epoch:    1/7     Loss: 4.27305552482605\n",
      "\n",
      "Epoch:    1/7     Loss: 4.27618450164795\n",
      "\n",
      "Epoch:    1/7     Loss: 4.19946576833725\n",
      "\n",
      "Epoch:    1/7     Loss: 4.24986569404602\n",
      "\n",
      "Epoch:    1/7     Loss: 4.2374523258209225\n",
      "\n",
      "Epoch:    1/7     Loss: 4.214609980583191\n",
      "\n",
      "Epoch:    1/7     Loss: 4.235961711406707\n",
      "\n",
      "Epoch:    1/7     Loss: 4.217500841617584\n",
      "\n",
      "Epoch:    1/7     Loss: 4.181265099048614\n",
      "\n",
      "Epoch:    1/7     Loss: 4.196170296669006\n",
      "\n",
      "Epoch:    1/7     Loss: 4.172747151851654\n",
      "\n",
      "Epoch:    1/7     Loss: 4.139363520145416\n",
      "\n",
      "Epoch:    1/7     Loss: 4.141070895195007\n",
      "\n",
      "Epoch:    1/7     Loss: 4.140413651466369\n",
      "\n",
      "Epoch:    1/7     Loss: 4.177115738391876\n",
      "\n",
      "Epoch:    1/7     Loss: 4.130230355262756\n",
      "\n",
      "Epoch:    1/7     Loss: 4.136950285434723\n",
      "\n",
      "Epoch:    2/7     Loss: 4.034225716539051\n",
      "\n",
      "Epoch:    2/7     Loss: 3.9651137495040896\n",
      "\n",
      "Epoch:    2/7     Loss: 4.001637845039368\n",
      "\n",
      "Epoch:    2/7     Loss: 3.9881470084190367\n",
      "\n",
      "Epoch:    2/7     Loss: 3.9552818870544435\n",
      "\n",
      "Epoch:    2/7     Loss: 3.995151219367981\n",
      "\n",
      "Epoch:    2/7     Loss: 3.984595537185669\n",
      "\n",
      "Epoch:    2/7     Loss: 3.9602298021316527\n",
      "\n",
      "Epoch:    2/7     Loss: 3.965496525764465\n",
      "\n",
      "Epoch:    2/7     Loss: 3.969203007221222\n",
      "\n",
      "Epoch:    2/7     Loss: 3.9944163155555725\n",
      "\n",
      "Epoch:    2/7     Loss: 3.992092845439911\n",
      "\n",
      "Epoch:    2/7     Loss: 3.965828583240509\n",
      "\n",
      "Epoch:    2/7     Loss: 3.9686277747154235\n",
      "\n",
      "Epoch:    2/7     Loss: 3.977591106891632\n",
      "\n",
      "Epoch:    2/7     Loss: 3.942923309803009\n",
      "\n",
      "Epoch:    2/7     Loss: 3.9707594537734985\n",
      "\n",
      "Epoch:    2/7     Loss: 3.9645686674118044\n",
      "\n",
      "Epoch:    2/7     Loss: 3.946651041507721\n",
      "\n",
      "Epoch:    2/7     Loss: 3.9509981298446655\n",
      "\n",
      "Epoch:    2/7     Loss: 3.938782868385315\n",
      "\n",
      "Epoch:    2/7     Loss: 3.9096132254600526\n",
      "\n",
      "Epoch:    2/7     Loss: 3.8744405913352966\n",
      "\n",
      "Epoch:    2/7     Loss: 3.898336503505707\n",
      "\n",
      "Epoch:    2/7     Loss: 3.9447246527671815\n",
      "\n",
      "Epoch:    2/7     Loss: 3.943256618976593\n",
      "\n",
      "Epoch:    2/7     Loss: 3.9324100375175477\n",
      "\n",
      "Epoch:    2/7     Loss: 3.9010769867897035\n",
      "\n",
      "Epoch:    2/7     Loss: 3.9168870520591734\n",
      "\n",
      "Epoch:    2/7     Loss: 3.9255378007888795\n",
      "\n",
      "Epoch:    2/7     Loss: 3.924571487903595\n",
      "\n",
      "Epoch:    2/7     Loss: 3.9205244040489196\n",
      "\n",
      "Epoch:    2/7     Loss: 3.8862349724769594\n",
      "\n",
      "Epoch:    2/7     Loss: 3.912005033493042\n",
      "\n",
      "Epoch:    3/7     Loss: 3.819233115600503\n",
      "\n",
      "Epoch:    3/7     Loss: 3.7270488643646242\n",
      "\n",
      "Epoch:    3/7     Loss: 3.7352093386650087\n",
      "\n",
      "Epoch:    3/7     Loss: 3.716241319179535\n",
      "\n",
      "Epoch:    3/7     Loss: 3.75631697177887\n",
      "\n",
      "Epoch:    3/7     Loss: 3.7666647911071776\n",
      "\n",
      "Epoch:    3/7     Loss: 3.7467575740814207\n",
      "\n",
      "Epoch:    3/7     Loss: 3.7488040351867675\n",
      "\n",
      "Epoch:    3/7     Loss: 3.7742040824890135\n",
      "\n",
      "Epoch:    3/7     Loss: 3.7936138200759886\n",
      "\n",
      "Epoch:    3/7     Loss: 3.6982769894599916\n",
      "\n",
      "Epoch:    3/7     Loss: 3.7606363105773926\n",
      "\n",
      "Epoch:    3/7     Loss: 3.7341169691085816\n",
      "\n",
      "Epoch:    3/7     Loss: 3.740580060482025\n",
      "\n",
      "Epoch:    3/7     Loss: 3.7495143818855285\n",
      "\n",
      "Epoch:    3/7     Loss: 3.757800943851471\n",
      "\n",
      "Epoch:    3/7     Loss: 3.7613131284713743\n",
      "\n",
      "Epoch:    3/7     Loss: 3.777929649353027\n",
      "\n",
      "Epoch:    3/7     Loss: 3.780300867557526\n",
      "\n",
      "Epoch:    3/7     Loss: 3.7563867163658142\n",
      "\n",
      "Epoch:    3/7     Loss: 3.7106578731536866\n",
      "\n",
      "Epoch:    3/7     Loss: 3.7218277621269227\n",
      "\n",
      "Epoch:    3/7     Loss: 3.7846625232696534\n",
      "\n",
      "Epoch:    3/7     Loss: 3.751437849998474\n",
      "\n",
      "Epoch:    3/7     Loss: 3.7881629347801207\n",
      "\n",
      "Epoch:    3/7     Loss: 3.7429533076286314\n",
      "\n",
      "Epoch:    3/7     Loss: 3.7085885906219485\n",
      "\n",
      "Epoch:    3/7     Loss: 3.6986631989479064\n",
      "\n",
      "Epoch:    3/7     Loss: 3.7308422756195068\n",
      "\n",
      "Epoch:    3/7     Loss: 3.7539530277252195\n",
      "\n",
      "Epoch:    3/7     Loss: 3.7356261587142945\n",
      "\n",
      "Epoch:    3/7     Loss: 3.7603825330734253\n",
      "\n",
      "Epoch:    3/7     Loss: 3.7276904702186586\n",
      "\n",
      "Epoch:    3/7     Loss: 3.7290622210502624\n",
      "\n",
      "Epoch:    4/7     Loss: 3.6241321913574054\n",
      "\n",
      "Epoch:    4/7     Loss: 3.5908218359947206\n",
      "\n",
      "Epoch:    4/7     Loss: 3.576921966075897\n",
      "\n",
      "Epoch:    4/7     Loss: 3.5972633504867555\n",
      "\n",
      "Epoch:    4/7     Loss: 3.553449568748474\n",
      "\n",
      "Epoch:    4/7     Loss: 3.5612391781806947\n",
      "\n",
      "Epoch:    4/7     Loss: 3.5542008972167967\n",
      "\n",
      "Epoch:    4/7     Loss: 3.5702468252182005\n",
      "\n",
      "Epoch:    4/7     Loss: 3.570570478439331\n",
      "\n",
      "Epoch:    4/7     Loss: 3.535945703983307\n",
      "\n",
      "Epoch:    4/7     Loss: 3.589295678138733\n",
      "\n",
      "Epoch:    4/7     Loss: 3.5688773465156554\n",
      "\n",
      "Epoch:    4/7     Loss: 3.5882620811462402\n",
      "\n",
      "Epoch:    4/7     Loss: 3.554508879184723\n",
      "\n",
      "Epoch:    4/7     Loss: 3.573962526321411\n",
      "\n",
      "Epoch:    4/7     Loss: 3.5599262833595278\n",
      "\n",
      "Epoch:    4/7     Loss: 3.5950708055496214\n",
      "\n",
      "Epoch:    4/7     Loss: 3.5862080001831056\n",
      "\n",
      "Epoch:    4/7     Loss: 3.5697529196739195\n",
      "\n",
      "Epoch:    4/7     Loss: 3.583079252243042\n",
      "\n",
      "Epoch:    4/7     Loss: 3.558503842353821\n",
      "\n",
      "Epoch:    4/7     Loss: 3.6325469613075256\n",
      "\n",
      "Epoch:    4/7     Loss: 3.618294198513031\n",
      "\n",
      "Epoch:    4/7     Loss: 3.6205877757072447\n",
      "\n",
      "Epoch:    4/7     Loss: 3.5855796051025393\n",
      "\n",
      "Epoch:    4/7     Loss: 3.588221859931946\n",
      "\n",
      "Epoch:    4/7     Loss: 3.5952251100540162\n",
      "\n",
      "Epoch:    4/7     Loss: 3.6164540815353394\n",
      "\n",
      "Epoch:    4/7     Loss: 3.5884138321876526\n",
      "\n",
      "Epoch:    4/7     Loss: 3.6157985019683836\n",
      "\n",
      "Epoch:    4/7     Loss: 3.606058611869812\n",
      "\n",
      "Epoch:    4/7     Loss: 3.601939573287964\n",
      "\n",
      "Epoch:    4/7     Loss: 3.5872010159492493\n",
      "\n",
      "Epoch:    4/7     Loss: 3.594856908321381\n",
      "\n",
      "Epoch:    5/7     Loss: 3.502659946680069\n",
      "\n",
      "Epoch:    5/7     Loss: 3.4286315512657164\n",
      "\n",
      "Epoch:    5/7     Loss: 3.408486032485962\n",
      "\n",
      "Epoch:    5/7     Loss: 3.407773253917694\n",
      "\n",
      "Epoch:    5/7     Loss: 3.4314067268371584\n",
      "\n",
      "Epoch:    5/7     Loss: 3.424754123687744\n",
      "\n",
      "Epoch:    5/7     Loss: 3.404824342727661\n",
      "\n",
      "Epoch:    5/7     Loss: 3.460716471672058\n",
      "\n",
      "Epoch:    5/7     Loss: 3.4051802492141725\n",
      "\n",
      "Epoch:    5/7     Loss: 3.4210390782356264\n",
      "\n",
      "Epoch:    5/7     Loss: 3.4229194855690004\n",
      "\n",
      "Epoch:    5/7     Loss: 3.4438062739372253\n",
      "\n",
      "Epoch:    5/7     Loss: 3.4408814096450806\n",
      "\n",
      "Epoch:    5/7     Loss: 3.42098605632782\n",
      "\n",
      "Epoch:    5/7     Loss: 3.45275057554245\n",
      "\n",
      "Epoch:    5/7     Loss: 3.454556441307068\n",
      "\n",
      "Epoch:    5/7     Loss: 3.4610309314727785\n",
      "\n",
      "Epoch:    5/7     Loss: 3.498866024017334\n",
      "\n",
      "Epoch:    5/7     Loss: 3.46175012588501\n",
      "\n",
      "Epoch:    5/7     Loss: 3.4416727137565615\n",
      "\n",
      "Epoch:    5/7     Loss: 3.4703082704544066\n",
      "\n",
      "Epoch:    5/7     Loss: 3.4322838854789732\n",
      "\n",
      "Epoch:    5/7     Loss: 3.4682375717163088\n",
      "\n",
      "Epoch:    5/7     Loss: 3.4680225229263306\n",
      "\n",
      "Epoch:    5/7     Loss: 3.4844810271263125\n",
      "\n",
      "Epoch:    5/7     Loss: 3.4830775380134584\n",
      "\n",
      "Epoch:    5/7     Loss: 3.4249374771118166\n",
      "\n",
      "Epoch:    5/7     Loss: 3.4694412612915038\n",
      "\n",
      "Epoch:    5/7     Loss: 3.462904105186462\n",
      "\n",
      "Epoch:    5/7     Loss: 3.5096606993675232\n",
      "\n",
      "Epoch:    5/7     Loss: 3.4606035923957825\n",
      "\n",
      "Epoch:    5/7     Loss: 3.4555235052108766\n",
      "\n",
      "Epoch:    5/7     Loss: 3.452359907627106\n",
      "\n",
      "Epoch:    5/7     Loss: 3.4739444851875305\n",
      "\n",
      "Epoch:    6/7     Loss: 3.377827561419943\n",
      "\n",
      "Epoch:    6/7     Loss: 3.2736380815505983\n",
      "\n",
      "Epoch:    6/7     Loss: 3.283183674812317\n",
      "\n",
      "Epoch:    6/7     Loss: 3.2991118741035463\n",
      "\n",
      "Epoch:    6/7     Loss: 3.283781521320343\n",
      "\n",
      "Epoch:    6/7     Loss: 3.293850846290588\n",
      "\n",
      "Epoch:    6/7     Loss: 3.2737511563301087\n",
      "\n",
      "Epoch:    6/7     Loss: 3.2985793685913087\n",
      "\n",
      "Epoch:    6/7     Loss: 3.289783253669739\n",
      "\n",
      "Epoch:    6/7     Loss: 3.3147096467018127\n",
      "\n",
      "Epoch:    6/7     Loss: 3.328301088809967\n",
      "\n",
      "Epoch:    6/7     Loss: 3.3296474266052245\n",
      "\n",
      "Epoch:    6/7     Loss: 3.2898439812660216\n",
      "\n",
      "Epoch:    6/7     Loss: 3.3193429780006407\n",
      "\n",
      "Epoch:    6/7     Loss: 3.3274384713172913\n",
      "\n",
      "Epoch:    6/7     Loss: 3.292595994472504\n",
      "\n",
      "Epoch:    6/7     Loss: 3.3550252723693847\n",
      "\n",
      "Epoch:    6/7     Loss: 3.3278508949279786\n",
      "\n",
      "Epoch:    6/7     Loss: 3.3217721676826475\n",
      "\n",
      "Epoch:    6/7     Loss: 3.340091338157654\n",
      "\n",
      "Epoch:    6/7     Loss: 3.3385475778579714\n",
      "\n",
      "Epoch:    6/7     Loss: 3.3483801436424256\n",
      "\n",
      "Epoch:    6/7     Loss: 3.3584122800827028\n",
      "\n",
      "Epoch:    6/7     Loss: 3.363702244758606\n",
      "\n",
      "Epoch:    6/7     Loss: 3.3611688947677614\n",
      "\n",
      "Epoch:    6/7     Loss: 3.3369200444221496\n",
      "\n",
      "Epoch:    6/7     Loss: 3.308999433517456\n",
      "\n",
      "Epoch:    6/7     Loss: 3.3567254304885865\n",
      "\n",
      "Epoch:    6/7     Loss: 3.3573943614959716\n",
      "\n",
      "Epoch:    6/7     Loss: 3.365859069824219\n",
      "\n",
      "Epoch:    6/7     Loss: 3.3506765937805176\n",
      "\n",
      "Epoch:    6/7     Loss: 3.380690379142761\n",
      "\n",
      "Epoch:    6/7     Loss: 3.3705403351783754\n",
      "\n",
      "Epoch:    6/7     Loss: 3.390483856201172\n",
      "\n",
      "Epoch:    7/7     Loss: 3.259127316267594\n",
      "\n",
      "Epoch:    7/7     Loss: 3.1251463985443113\n",
      "\n",
      "Epoch:    7/7     Loss: 3.1715997362136843\n",
      "\n",
      "Epoch:    7/7     Loss: 3.1504652762413023\n",
      "\n",
      "Epoch:    7/7     Loss: 3.155022418498993\n",
      "\n",
      "Epoch:    7/7     Loss: 3.184628167152405\n",
      "\n",
      "Epoch:    7/7     Loss: 3.1969791293144225\n",
      "\n",
      "Epoch:    7/7     Loss: 3.174297230243683\n",
      "\n",
      "Epoch:    7/7     Loss: 3.1892885851860044\n",
      "\n",
      "Epoch:    7/7     Loss: 3.19496789932251\n",
      "\n",
      "Epoch:    7/7     Loss: 3.195507810115814\n",
      "\n",
      "Epoch:    7/7     Loss: 3.226951024532318\n",
      "\n",
      "Epoch:    7/7     Loss: 3.1891051387786864\n",
      "\n",
      "Epoch:    7/7     Loss: 3.209349217414856\n",
      "\n",
      "Epoch:    7/7     Loss: 3.224496796131134\n",
      "\n",
      "Epoch:    7/7     Loss: 3.223923213481903\n",
      "\n",
      "Epoch:    7/7     Loss: 3.2231996512413024\n",
      "\n",
      "Epoch:    7/7     Loss: 3.240386354923248\n",
      "\n",
      "Epoch:    7/7     Loss: 3.2205959367752075\n",
      "\n",
      "Epoch:    7/7     Loss: 3.2328329920768737\n",
      "\n",
      "Epoch:    7/7     Loss: 3.2351199078559874\n",
      "\n",
      "Epoch:    7/7     Loss: 3.244033446311951\n",
      "\n",
      "Epoch:    7/7     Loss: 3.251088478565216\n",
      "\n",
      "Epoch:    7/7     Loss: 3.264810256958008\n",
      "\n",
      "Epoch:    7/7     Loss: 3.226441535949707\n",
      "\n",
      "Epoch:    7/7     Loss: 3.257718548774719\n",
      "\n",
      "Epoch:    7/7     Loss: 3.244812126159668\n",
      "\n",
      "Epoch:    7/7     Loss: 3.260105082988739\n",
      "\n",
      "Epoch:    7/7     Loss: 3.2620169568061828\n",
      "\n",
      "Epoch:    7/7     Loss: 3.2458088898658755\n",
      "\n",
      "Epoch:    7/7     Loss: 3.279306514263153\n",
      "\n",
      "Epoch:    7/7     Loss: 3.2478314876556396\n",
      "\n",
      "Epoch:    7/7     Loss: 3.269831540584564\n",
      "\n",
      "Epoch:    7/7     Loss: 3.25086971282959\n",
      "\n",
      "Model Trained and Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type RNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dJenBJipCmQH",
    "outputId": "196bcd36-2698-4104-99ac-6acd6633e2d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.0'"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k1JGA_uLCmQK"
   },
   "source": [
    "### 问题: 你如何决定你的模型超参数？\n",
    "比如，你是否试过不同的 different sequence_lengths 并发现哪个使得模型的收敛速度变化？那你的隐藏层数和层数呢？你是如何决定使用这个网络参数的？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "APDKA3s0CmQK"
   },
   "source": [
    "**答案:** \n",
    "\n",
    "我调整了各种参数, 比如sequence_lengths, 刚开始设置成100, 但发现效果不是很好. 后来又设置成20, 效果较好. \n",
    "\n",
    "我的hidden_dim设置成了512, embedding_dim设置成了256, n_layers设置为2, 这几个参数参数越大, 意味着模型越复杂, 在经过很多次调整和测试之后, 我发现这几个参数的设置是相对来说更合理的.\n",
    "\n",
    "我训练了7个epoch, 最终的loss值为3.25左右.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fqCI1J1nCmQL"
   },
   "source": [
    "*italicized text*---\n",
    "# 检查点\n",
    "\n",
    "通过运行上面的训练单元，你的模型已经以`trained_rnn`名字存储，如果你存储了你的notebook， **你可以在之后的任何时间来访问你的代码和结果**. 下述代码可以帮助你重载你的结果!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rQYOmSvvCmQM"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import torch\n",
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "_, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n",
    "trained_rnn = helper.load_model('./save/trained_rnn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1GeftylcCmQR"
   },
   "source": [
    "## 生成电视剧剧本\n",
    "你现在可以生成你的“假”电视剧剧本啦！\n",
    "\n",
    "### 生成文字\n",
    "你的神经网络会不断重复生成一个单词，直到生成满足你要求长度的剧本。使用 `generate` 函数来完成上述操作。首先，使用 `prime_id` 来生成word id，之后确定生成文本长度 `predict_len`。同时， topk 采样来引入文字选择的随机性!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t5HAc_HzCmQR"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def generate(rnn, prime_id, int_to_vocab, token_dict, pad_value, predict_len=100):\n",
    "    \"\"\"\n",
    "    Generate text using the neural network\n",
    "    :param decoder: The PyTorch Module that holds the trained neural network\n",
    "    :param prime_id: The word id to start the first prediction\n",
    "    :param int_to_vocab: Dict of word id keys to word values\n",
    "    :param token_dict: Dict of puncuation tokens keys to puncuation values\n",
    "    :param pad_value: The value used to pad a sequence\n",
    "    :param predict_len: The length of text to generate\n",
    "    :return: The generated text\n",
    "    \"\"\"\n",
    "    rnn.eval()\n",
    "    \n",
    "    # create a sequence (batch_size=1) with the prime_id\n",
    "    current_seq = np.full((1, sequence_length), pad_value)\n",
    "    current_seq[-1][-1] = prime_id\n",
    "    predicted = [int_to_vocab[prime_id]]\n",
    "    \n",
    "    for _ in range(predict_len):\n",
    "        if train_on_gpu:\n",
    "            current_seq = torch.LongTensor(current_seq).cuda()\n",
    "        else:\n",
    "          current_seq = torch.LongTensor(current_seq)\n",
    "        \n",
    "        # initialize the hidden state\n",
    "        hidden = rnn.init_hidden(current_seq.size(0))\n",
    "        \n",
    "        # get the output of the rnn\n",
    "        output, _ = rnn(current_seq, hidden)\n",
    "        \n",
    "        # get the next word probabilities\n",
    "        p = F.softmax(output, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "         \n",
    "        # use top_k sampling to get the index of the next word\n",
    "        top_k = 5\n",
    "        p, top_i = p.topk(top_k)\n",
    "        top_i = top_i.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next word index with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        word_i = np.random.choice(top_i, p=p/p.sum())\n",
    "        \n",
    "        # retrieve that word from the dictionary\n",
    "        word = int_to_vocab[word_i]\n",
    "        predicted.append(word)     \n",
    "        \n",
    "        # the generated word becomes the next \"current sequence\" and the cycle can continue\n",
    "        current_seq = np.roll(current_seq.cpu(), -1, 1)\n",
    "        current_seq[-1][-1] = word_i\n",
    "    \n",
    "    gen_sentences = ' '.join(predicted)\n",
    "    \n",
    "    # Replace punctuation tokens\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        gen_sentences = gen_sentences.replace(' ' + token.lower(), key)\n",
    "    gen_sentences = gen_sentences.replace('\\n ', '\\n')\n",
    "    gen_sentences = gen_sentences.replace('( ', '(')\n",
    "    \n",
    "    # return all the sentences\n",
    "    return gen_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YRryUfDHCmQV"
   },
   "source": [
    "### 生成一个新剧本\n",
    "是时候生成一个剧本啦。设置`gen_length` 剧本长度，设置 `prime_word`为以下任意词来开始生成吧:\n",
    "- \"jerry\"\n",
    "- \"elaine\"\n",
    "- \"george\"\n",
    "- \"kramer\"\n",
    "\n",
    "你可以把prime word 设置成 _任意 _ 单词, 但是使用名字开始会比较好(任何其他名字也是可以哒!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "F8ACQCJ6CmQY",
    "outputId": "0584af75-e062-4f93-fd26-99da7a1db5ee",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jerry:.\n",
      "\n",
      "elaine: what is this? i thought you liked her?\n",
      "\n",
      "george: no.\n",
      "\n",
      "jerry:(smiling) i don't want to get out of here.\n",
      "\n",
      "george:(smiling, not) oh, no!\n",
      "\n",
      "george: i don't know what to do, you know, it's a good idea.\n",
      "\n",
      "jerry: you don't know how you feel?\n",
      "\n",
      "elaine:(quietly).(to kramer)\n",
      "\n",
      "kramer:(quietly)\n",
      "\n",
      "jerry:(on the phone) yeah, that's what they do.\n",
      "\n",
      "kramer: oh, yeah.\n",
      "\n",
      "elaine: you got that?\n",
      "\n",
      "jerry:(looking up to the van) hey, you got it?\n",
      "\n",
      "jerry:(to the manager of the conversation) oh, you don't have to do that.\n",
      "\n",
      "elaine: you have to go to the bathroom.(george shakes his head)\n",
      "\n",
      "elaine: oh! i think you should see that.\n",
      "\n",
      "george:(to the waitress) yeah.\n",
      "\n",
      "jerry:(to kramer) hey, you know what? i was just trying to get away, i know what i'm saying.\n",
      "\n",
      "kramer: i don't have to...\n",
      "\n",
      "george:(quietly) oh, you got it.\n",
      "\n",
      "jerry: oh, hi.\n",
      "\n",
      "jerry: hi.\n",
      "\n",
      "george:(to jerry) well, i don't know.\n",
      "\n",
      "jerry:(to george) i know... i know...\n",
      "\n",
      "kramer: well, i don't want to know if i can do something.\n",
      "\n",
      "george: what do you think?\n",
      "\n",
      "jerry: yeah.\n",
      "\n",
      "jerry:(looking over the speaker) you are the same.\n",
      "\n",
      "jerry: i can't go back to the game.\n",
      "\n",
      "elaine: i think you can get it in the car!\n",
      "\n",
      "jerry: oh, no no, i don't know. i'm just trying to get some sleep.\n",
      "\n",
      "jerry: oh, i was just thinking of a woman.\n",
      "\n",
      "jerry: i don't know...\n",
      "\n",
      "george: you know the last one of these guys are going to be a lot of money.(he exits the street to the kitchen)\n",
      "\n",
      "kramer:(pointing at him) hey, what are you doing with this?\n",
      "\n",
      "jerry: what are you gonna tell you?\n",
      "\n",
      "jerry: what?\n",
      "\n",
      "elaine: well, you know, i think i'm sorry.\n",
      "\n",
      "elaine: well, i just came from the airport.\n",
      "\n",
      "george: i know, it's not the point.(she starts to get away)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run the cell multiple times to get different results!\n",
    "gen_length = 500 # modify the length to your preference\n",
    "prime_word = 'jerry' # name for starting the script\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "pad_word = helper.SPECIAL_WORDS['PADDING']\n",
    "generated_script = generate(trained_rnn, vocab_to_int[prime_word + ':'], int_to_vocab, token_dict, vocab_to_int[pad_word], gen_length)\n",
    "print(generated_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yXXH_4ynCmQb"
   },
   "source": [
    "#### 存下你最爱的片段\n",
    "\n",
    "一旦你发现一段有趣或者好玩的片段，就把它存下啦！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F2m0G6XCCmQc"
   },
   "outputs": [],
   "source": [
    "# save script to a text file\n",
    "f =  open(\"generated_script_1.txt\",\"w\")\n",
    "f.write(generated_script)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6-mUPYLXCmQd"
   },
   "source": [
    "# 这个电视剧剧本是无意义的\n",
    "如果你的电视剧剧本不是很有逻辑也是ok的。下面是一个例子。\n",
    "\n",
    "### 生成剧本案例\n",
    "\n",
    ">jerry: what about me?\n",
    ">\n",
    ">jerry: i don't have to wait.\n",
    ">\n",
    ">kramer:(to the sales table)\n",
    ">\n",
    ">elaine:(to jerry) hey, look at this, i'm a good doctor.\n",
    ">\n",
    ">newman:(to elaine) you think i have no idea of this...\n",
    ">\n",
    ">elaine: oh, you better take the phone, and he was a little nervous.\n",
    ">\n",
    ">kramer:(to the phone) hey, hey, jerry, i don't want to be a little bit.(to kramer and jerry) you can't.\n",
    ">\n",
    ">jerry: oh, yeah. i don't even know, i know.\n",
    ">\n",
    ">jerry:(to the phone) oh, i know.\n",
    ">\n",
    ">kramer:(laughing) you know...(to jerry) you don't know.\n",
    "\n",
    "\n",
    "如果这个电视剧剧本毫无意义，那也没有关系。我们的训练文本不到一兆字节。为了获得更好的结果，你需要使用更小的词汇范围或是更多数据。幸运的是，我们的确拥有更多数据！在本项目开始之初我们也曾提过，这是[另一个数据集](https://www.kaggle.com/wcukierski/the-simpsons-by-the-data)的子集。我们并没有让你基于所有数据进行训练，因为这将耗费大量时间。然而，你可以随意使用这些数据训练你的神经网络。当然，是在完成本项目之后。\n",
    "# 提交项目\n",
    "在提交项目时，请确保你在保存 notebook 前运行了所有的单元格代码。请将 notebook 文件保存为 \"dlnd_tv_script_generation.ipynb\"，并将它作为 HTML 文件保存在 \"File\" -> \"Download as\" 中。请将 \"helper.py\" 和 \"problem_unittests.py\" 文件一并打包成 zip 文件提交。"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "J5PuEkB7CmPg",
    "dJK5vTWKCmPq"
   ],
   "name": "dlnd_tv_script_generation_zh.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

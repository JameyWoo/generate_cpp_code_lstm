{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "lstm_generate_cpp_code.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "widgets": {
      "state": {},
      "version": "1.1.2"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uyYpfpgjRouH",
        "colab": {}
      },
      "source": [
        "!unzip lstm-生成cpp代码.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uCZbvutfCmOL",
        "outputId": "7399af43-de07-4104-edef-adef93bd3145",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "assets\t\tlstm_generate_cpp_code.ipynb  sample_data\n",
            "generate\tlstm-生成cpp代码.zip\t      splitCppData.py\n",
            "getCppFiles.py\t__pycache__\t\t      tempCodeRunnerFile.py\n",
            "helper.py\trandomSequence.py\t      trained_rnn_1.pt\n",
            "imgs\t\treadme.md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TtdAQvctvq2K",
        "outputId": "4ed2267a-101e-4d34-e813-73fa9c67d908",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from splitCppData import splitData\n",
        "\n",
        "char2int, int2char, int_text, max_int = splitData()\n",
        "print(len(int_text), max_int)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2691 9543\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8XfWU2ydwd_N",
        "outputId": "52fb902a-beb1-4297-d0c4-fe2588ee4e66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "maxx = 0\n",
        "for each in int_text:\n",
        "    for any in each:\n",
        "        maxx = max(maxx, any)\n",
        "\n",
        "print(maxx)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9543\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YWDv6DzsP9Kd",
        "outputId": "1a9e2ae8-6874-4e0d-afdf-f03c77b32bd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from randomSequence import getTrainLoader\n",
        "\n",
        "train_loader = getTrainLoader()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "766502\n",
            "766502\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QO6cBoWHP9Kh",
        "outputId": "ef6b5db3-e2a1-41e2-a315-c920b7462726",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "max(max(int_text))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7181"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "20TX-TCRP9Km",
        "outputId": "a6075657-c3ce-43bc-a11f-fa3dfeb9dff6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "# Check for a GPU\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "if not train_on_gpu:\n",
        "    print('No GPU found. Please use a GPU to train your neural network.')\n",
        "else:\n",
        "    print('GPU is prepared!')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU is prepared!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TRg8lCu1CmPn",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5):\n",
        "        \"\"\"\n",
        "        Initialize the PyTorch RNN Module\n",
        "        :param vocab_size: The number of input dimensions of the neural network (the size of the vocabulary)\n",
        "        :param output_size: The number of output dimensions of the neural network\n",
        "        :param embedding_dim: The size of embeddings, should you choose to use them        \n",
        "        :param hidden_dim: The size of the hidden layer outputs\n",
        "        :param dropout: dropout to add in between LSTM/GRU layers\n",
        "        \"\"\"\n",
        "        super(RNN, self).__init__()\n",
        "        # TODO: Implement function\n",
        "        \n",
        "        # set class variables\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        # define model layers\n",
        "        self.embedding = nn.Embedding(vocab_size + 1, embedding_dim)\n",
        "        # 之前是vocab_size而不是vocab_size + 1. 一直报错: RuntimeError: index out of range at /opt/conda/conda-bld/pytorch_1524584710464/work/aten/src/TH/gene.\n",
        "        # 参考: https://blog.csdn.net/Geek_of_CSDN/article/details/86527107\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
        "                           dropout=dropout, batch_first=True)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        self.sig = nn.Sigmoid()\n",
        "    \n",
        "    \n",
        "    def forward(self, nn_input, hidden):\n",
        "        \"\"\"\n",
        "        Forward propagation of the neural network\n",
        "        :param nn_input: The input to the neural network\n",
        "        :param hidden: The hidden state        \n",
        "        :return: Two Tensors, the output of the neural network and the latest hidden state\n",
        "        \"\"\"\n",
        "        # TODO: Implement function   \n",
        "        x = nn_input\n",
        "        \n",
        "        batch_size = x.size(0)\n",
        "        \n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "        \n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "        \n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out)\n",
        "        \n",
        "        # sig_out = self.sig(out)\n",
        "        # 根据交流群同学的提示, 这里要取消sigmoid. 记得之前做卷积神经网络项目的时候也出现过这个问题\n",
        "        sig_out = out\n",
        "        \n",
        "        sig_out = sig_out.view(batch_size, -1, self.output_size)\n",
        "        sig_out = sig_out[:, -1]\n",
        "        \n",
        "        # return one batch of output word scores and the hidden state\n",
        "        return sig_out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        '''\n",
        "        Initialize the hidden state of an LSTM/GRU\n",
        "        :param batch_size: The batch_size of the hidden state\n",
        "        :return: hidden state of dims (n_layers, batch_size, hidden_dim)\n",
        "        '''\n",
        "        # Implement function\n",
        "        \n",
        "        # initialize hidden state with zero weights, and move to GPU if available\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if train_on_gpu:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
        "                     weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
        "        \n",
        "        return hidden\n",
        "\n",
        "# \"\"\"\n",
        "# DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
        "# \"\"\"\n",
        "# tests.test_rnn(RNN, train_on_gpu)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dJK5vTWKCmPq"
      },
      "source": [
        "### 定义前向及后向传播\n",
        "\n",
        "通过你实现的 RNN 类来进行前向及后项传播。你可以在训练循环中，不断地调用如下代码来实现：\n",
        "```\n",
        "loss, hidden = forward_back_prop(decoder, decoder_optimizer, criterion, inp, target)\n",
        "```\n",
        "\n",
        "函数中需要返回一个批次以及其隐藏状态的loss均值，你可以调用一个函数`RNN(inp, hidden)`来实现。记得，你可以通过调用`loss.item()` 来计算得到该loss。\n",
        "\n",
        "**如果使用 GPU，你需要将你的数据存到 GPU 的设备上。**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hx9r-9hGCmPq",
        "colab": {}
      },
      "source": [
        "def forward_back_prop(rnn, optimizer, criterion, inp, target, hidden):\n",
        "    \"\"\"\n",
        "    Forward and backward propagation on the neural network\n",
        "    :param decoder: The PyTorch Module that holds the neural network\n",
        "    :param decoder_optimizer: The PyTorch optimizer for the neural network\n",
        "    :param criterion: The PyTorch loss function\n",
        "    :param inp: A batch of input to the neural network\n",
        "    :param target: The target output for the batch of input\n",
        "    :return: The loss and the latest hidden state Tensor\n",
        "    \"\"\"\n",
        "    \n",
        "    # TODO: Implement Function\n",
        "    \n",
        "    # move data to GPU, if available\n",
        "    if train_on_gpu:\n",
        "        inp, target = inp.cuda(), target.cuda()\n",
        "    \n",
        "    # perform backpropagation and optimization\n",
        "    rnn.zero_grad()\n",
        "    # 修改\n",
        "    hidden = tuple([each.data for each in hidden])\n",
        "    output, hidden = rnn(inp, hidden)\n",
        "    # 修改\n",
        "    loss = criterion(output, target.long())\n",
        "    loss.backward()\n",
        "    \n",
        "    optimizer.step()\n",
        "\n",
        "    # return the loss over a batch and the hidden state produced by our model\n",
        "    return loss.item(), hidden\n",
        "\n",
        "# Note that these tests aren't completely extensive.\n",
        "# they are here to act as general checks on the expected outputs of your functions\n",
        "# \"\"\"\n",
        "# DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
        "# \"\"\"\n",
        "# tests.test_forward_back_prop(RNN, forward_back_prop, train_on_gpu)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AgBJRweXCmPs"
      },
      "source": [
        "## 神经网络训练\n",
        "\n",
        "神经网络结构完成以及数据准备完后，我们可以开始训练网络了。\n",
        "\n",
        "### 训练循环\n",
        "\n",
        "训练循环是通过 `train_decoder` 函数实现的。该函数将进行 epochs 次数的训练。模型的训练成果会在一定批次的训练后，被打印出来。这个“一定批次”可以通过`show_every_n_batches` 来设置。你会在下一节设置这个参数。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mZvsfb1jCmPt",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "DON'T MODIFY ANYTHING IN THIS CELL\n",
        "\"\"\"\n",
        "\n",
        "def train_rnn(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches=100):\n",
        "    batch_losses = []\n",
        "    \n",
        "    rnn.train()\n",
        "\n",
        "    print(\"Training for %d epoch(s)...\" % n_epochs)\n",
        "    for epoch_i in range(1, n_epochs + 1):\n",
        "        \n",
        "        # initialize hidden state\n",
        "        hidden = rnn.init_hidden(batch_size)\n",
        "        \n",
        "        for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n",
        "            \n",
        "            # make sure you iterate over completely full batches, only\n",
        "            n_batches = len(train_loader.dataset)//batch_size\n",
        "            if(batch_i > n_batches):\n",
        "                break\n",
        "            \n",
        "            # forward, back prop\n",
        "            loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden)          \n",
        "            # record loss\n",
        "            batch_losses.append(loss)\n",
        "\n",
        "            # printing loss stats\n",
        "            if batch_i % show_every_n_batches == 0:\n",
        "                if batch_i % 100 == 0:\n",
        "                    helper.save_model('./trained_rnn' + str(batch_i), rnn)\n",
        "                print('Epoch: {:>4}/{:<4}  Loss: {}\\n'.format(\n",
        "                    epoch_i, n_epochs, np.average(batch_losses)))\n",
        "                batch_losses = []\n",
        "\n",
        "    # returns a trained rnn\n",
        "    return rnn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KbaaWMpqCmPv"
      },
      "source": [
        "### 超参数\n",
        "\n",
        "设置并训练以下超参数:\n",
        "-  `sequence_length`，序列长度 \n",
        "-  `batch_size`，分批大小\n",
        "-  `num_epochs`，循环次数\n",
        "-  `learning_rate`，Adam优化器的学习率\n",
        "-  `vocab_size`，唯一标示词汇的数量\n",
        "-  `output_size`，模型输出的大小 \n",
        "-  `embedding_dim`，词嵌入的维度，小于 vocab_size\n",
        "-  `hidden_dim`， 隐藏层维度\n",
        "-  `n_layers`， RNN的层数\n",
        "-  `show_every_n_batches`，打印结果的频次\n",
        "\n",
        "如果模型没有获得你预期的结果，调整 `RNN`类中的上述参数。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6MtYhJhDCmPw",
        "colab": {}
      },
      "source": [
        "# Data params\n",
        "# Sequence Length\n",
        "sequence_length = 20  # of words in a sequence\n",
        "# Batch Size\n",
        "batch_size = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Yz2HzHhpCmP2",
        "colab": {}
      },
      "source": [
        "# Training parameters\n",
        "# Number of Epochs\n",
        "num_epochs = 1\n",
        "# Learning Rate\n",
        "learning_rate = 0.0018\n",
        "\n",
        "# Model parameters\n",
        "# Vocab size\n",
        "vocab_size = max_int + 1  # 词的数量, 从0开始索引, 所以需要+1\n",
        "# Output size\n",
        "output_size = vocab_size\n",
        "# Embedding Dimension\n",
        "embedding_dim = 256\n",
        "# Hidden Dimension\n",
        "hidden_dim = 512\n",
        "# Number of RNN Layers\n",
        "n_layers = 2\n",
        "\n",
        "# Show stats for every n number of batches\n",
        "show_every_n_batches = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E41Kb01uCmP6"
      },
      "source": [
        "### 训练\n",
        "下一节，通过预处理数据来训练神经网络。如果你的loss结果不好，可以通过调整超参数来修正。通常情况下，大的隐藏层及层数会带来比较好的效果，但同时也会消耗较长的时间来训练。\n",
        "> **你应该努力得到一个低于3.5的loss** \n",
        "\n",
        "你也可以试试不同的序列长度，该参数表明模型学习的范围大小。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PUzUpg8YSJue",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import helper"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R3UaTuwMCmP7",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "def run():\n",
        "    \"\"\"\n",
        "    DON'T MODIFY ANYTHING IN THIS CELL\n",
        "    \"\"\"\n",
        "\n",
        "    # create model and move to gpu if available\n",
        "    rnn = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.0)\n",
        "    if train_on_gpu:\n",
        "        rnn.cuda()\n",
        "\n",
        "    # defining loss and optimization functions for training\n",
        "    optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # training the model\n",
        "    trained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, show_every_n_batches)\n",
        "\n",
        "    # saving the trained model\n",
        "    helper.save_model('./trained_rnn', trained_rnn)\n",
        "    print('Model Trained and Saved')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1GFGgEhRo0Vg",
        "outputId": "fa3672a0-305e-43a6-fe5d-c79a6cb6ca45",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "run()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for 1 epoch(s)...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type RNN. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:    1/1     Loss: 6.724790143966675\n",
            "\n",
            "Epoch:    1/1     Loss: 5.373217916488647\n",
            "\n",
            "Epoch:    1/1     Loss: 5.0091150283813475\n",
            "\n",
            "Epoch:    1/1     Loss: 4.968357992172241\n",
            "\n",
            "Epoch:    1/1     Loss: 4.847093152999878\n",
            "\n",
            "Epoch:    1/1     Loss: 4.915909004211426\n",
            "\n",
            "Epoch:    1/1     Loss: 4.835130214691162\n",
            "\n",
            "Epoch:    1/1     Loss: 4.8013081550598145\n",
            "\n",
            "Epoch:    1/1     Loss: 4.773309421539307\n",
            "\n",
            "Epoch:    1/1     Loss: 4.760389804840088\n",
            "\n",
            "Epoch:    1/1     Loss: 4.714656972885132\n",
            "\n",
            "Epoch:    1/1     Loss: 4.504742097854614\n",
            "\n",
            "Epoch:    1/1     Loss: 4.436546444892883\n",
            "\n",
            "Epoch:    1/1     Loss: 4.294315767288208\n",
            "\n",
            "Epoch:    1/1     Loss: 4.107592248916626\n",
            "\n",
            "Epoch:    1/1     Loss: 4.161919784545899\n",
            "\n",
            "Epoch:    1/1     Loss: 3.8466867208480835\n",
            "\n",
            "Epoch:    1/1     Loss: 3.6953810930252073\n",
            "\n",
            "Epoch:    1/1     Loss: 3.6682599067687987\n",
            "\n",
            "Epoch:    1/1     Loss: 3.474245381355286\n",
            "\n",
            "Epoch:    1/1     Loss: 3.23008074760437\n",
            "\n",
            "Epoch:    1/1     Loss: 3.2118019819259644\n",
            "\n",
            "Epoch:    1/1     Loss: 3.2235686779022217\n",
            "\n",
            "Epoch:    1/1     Loss: 3.0591973781585695\n",
            "\n",
            "Epoch:    1/1     Loss: 3.1527074813842773\n",
            "\n",
            "Epoch:    1/1     Loss: 3.1682220697402954\n",
            "\n",
            "Epoch:    1/1     Loss: 3.008568835258484\n",
            "\n",
            "Epoch:    1/1     Loss: 2.9456807136535645\n",
            "\n",
            "Epoch:    1/1     Loss: 2.8254679441452026\n",
            "\n",
            "Epoch:    1/1     Loss: 2.963884973526001\n",
            "\n",
            "Epoch:    1/1     Loss: 2.9923930406570434\n",
            "\n",
            "Epoch:    1/1     Loss: 2.7118934869766234\n",
            "\n",
            "Epoch:    1/1     Loss: 2.7246230602264405\n",
            "\n",
            "Epoch:    1/1     Loss: 2.7225502014160154\n",
            "\n",
            "Epoch:    1/1     Loss: 2.708023738861084\n",
            "\n",
            "Epoch:    1/1     Loss: 2.764461541175842\n",
            "\n",
            "Epoch:    1/1     Loss: 2.7755565643310547\n",
            "\n",
            "Epoch:    1/1     Loss: 2.725442624092102\n",
            "\n",
            "Epoch:    1/1     Loss: 2.773995780944824\n",
            "\n",
            "Epoch:    1/1     Loss: 2.646158790588379\n",
            "\n",
            "Epoch:    1/1     Loss: 2.805650758743286\n",
            "\n",
            "Epoch:    1/1     Loss: 2.771643137931824\n",
            "\n",
            "Epoch:    1/1     Loss: 2.731883716583252\n",
            "\n",
            "Epoch:    1/1     Loss: 2.784557247161865\n",
            "\n",
            "Epoch:    1/1     Loss: 2.763650417327881\n",
            "\n",
            "Epoch:    1/1     Loss: 2.7214730978012085\n",
            "\n",
            "Epoch:    1/1     Loss: 2.5958271980285645\n",
            "\n",
            "Epoch:    1/1     Loss: 2.624962830543518\n",
            "\n",
            "Epoch:    1/1     Loss: 2.6222347021102905\n",
            "\n",
            "Epoch:    1/1     Loss: 2.6494779348373414\n",
            "\n",
            "Epoch:    1/1     Loss: 2.6353040456771852\n",
            "\n",
            "Epoch:    1/1     Loss: 2.484317111968994\n",
            "\n",
            "Epoch:    1/1     Loss: 2.5291418552398683\n",
            "\n",
            "Epoch:    1/1     Loss: 2.5120657682418823\n",
            "\n",
            "Epoch:    1/1     Loss: 2.4791470050811766\n",
            "\n",
            "Epoch:    1/1     Loss: 2.6099586725234984\n",
            "\n",
            "Epoch:    1/1     Loss: 2.4418023109436033\n",
            "\n",
            "Epoch:    1/1     Loss: 2.438238203525543\n",
            "\n",
            "Epoch:    1/1     Loss: 2.414694881439209\n",
            "\n",
            "Epoch:    1/1     Loss: 2.550154781341553\n",
            "\n",
            "Epoch:    1/1     Loss: 2.5474267482757567\n",
            "\n",
            "Epoch:    1/1     Loss: 2.550145411491394\n",
            "\n",
            "Epoch:    1/1     Loss: 2.5040807247161867\n",
            "\n",
            "Epoch:    1/1     Loss: 2.531900691986084\n",
            "\n",
            "Epoch:    1/1     Loss: 2.444792914390564\n",
            "\n",
            "Epoch:    1/1     Loss: 2.476960909366608\n",
            "\n",
            "Epoch:    1/1     Loss: 2.396383213996887\n",
            "\n",
            "Epoch:    1/1     Loss: 2.4103615045547486\n",
            "\n",
            "Epoch:    1/1     Loss: 2.383172106742859\n",
            "\n",
            "Epoch:    1/1     Loss: 2.3979998350143434\n",
            "\n",
            "Epoch:    1/1     Loss: 2.4885097980499267\n",
            "\n",
            "Epoch:    1/1     Loss: 2.4331814527511595\n",
            "\n",
            "Epoch:    1/1     Loss: 2.3588544964790343\n",
            "\n",
            "Epoch:    1/1     Loss: 2.4511638045310975\n",
            "\n",
            "Epoch:    1/1     Loss: 2.204416310787201\n",
            "\n",
            "Epoch:    1/1     Loss: 2.257913899421692\n",
            "\n",
            "Epoch:    1/1     Loss: 2.3853867053985596\n",
            "\n",
            "Epoch:    1/1     Loss: 2.2939172863960264\n",
            "\n",
            "Epoch:    1/1     Loss: 2.4472802877426147\n",
            "\n",
            "Epoch:    1/1     Loss: 2.432297205924988\n",
            "\n",
            "Epoch:    1/1     Loss: 2.357313632965088\n",
            "\n",
            "Epoch:    1/1     Loss: 2.4063127279281615\n",
            "\n",
            "Epoch:    1/1     Loss: 2.363146126270294\n",
            "\n",
            "Epoch:    1/1     Loss: 2.3783878326416015\n",
            "\n",
            "Epoch:    1/1     Loss: 2.457027053833008\n",
            "\n",
            "Epoch:    1/1     Loss: 2.511580467224121\n",
            "\n",
            "Epoch:    1/1     Loss: 2.440120816230774\n",
            "\n",
            "Epoch:    1/1     Loss: 2.3240697741508485\n",
            "\n",
            "Epoch:    1/1     Loss: 2.4088302135467528\n",
            "\n",
            "Epoch:    1/1     Loss: 2.289359748363495\n",
            "\n",
            "Epoch:    1/1     Loss: 2.4096837997436524\n",
            "\n",
            "Epoch:    1/1     Loss: 2.246477949619293\n",
            "\n",
            "Epoch:    1/1     Loss: 2.3740431308746337\n",
            "\n",
            "Epoch:    1/1     Loss: 2.2703084349632263\n",
            "\n",
            "Epoch:    1/1     Loss: 2.327621030807495\n",
            "\n",
            "Epoch:    1/1     Loss: 2.306711709499359\n",
            "\n",
            "Epoch:    1/1     Loss: 2.319578695297241\n",
            "\n",
            "Epoch:    1/1     Loss: 2.2413835525512695\n",
            "\n",
            "Epoch:    1/1     Loss: 2.444573473930359\n",
            "\n",
            "Epoch:    1/1     Loss: 2.3749568700790404\n",
            "\n",
            "Epoch:    1/1     Loss: 2.2608270406723023\n",
            "\n",
            "Epoch:    1/1     Loss: 2.3131675839424135\n",
            "\n",
            "Epoch:    1/1     Loss: 2.218137192726135\n",
            "\n",
            "Epoch:    1/1     Loss: 2.298888611793518\n",
            "\n",
            "Epoch:    1/1     Loss: 2.218321704864502\n",
            "\n",
            "Epoch:    1/1     Loss: 2.384249758720398\n",
            "\n",
            "Epoch:    1/1     Loss: 2.2388363122940063\n",
            "\n",
            "Epoch:    1/1     Loss: 2.1084948897361757\n",
            "\n",
            "Epoch:    1/1     Loss: 2.375392961502075\n",
            "\n",
            "Epoch:    1/1     Loss: 2.248392105102539\n",
            "\n",
            "Epoch:    1/1     Loss: 2.2477084755897523\n",
            "\n",
            "Epoch:    1/1     Loss: 2.2425514340400694\n",
            "\n",
            "Epoch:    1/1     Loss: 2.3526798367500303\n",
            "\n",
            "Epoch:    1/1     Loss: 2.1195909857749937\n",
            "\n",
            "Epoch:    1/1     Loss: 2.2531713247299194\n",
            "\n",
            "Epoch:    1/1     Loss: 2.349517619609833\n",
            "\n",
            "Epoch:    1/1     Loss: 2.283330798149109\n",
            "\n",
            "Epoch:    1/1     Loss: 2.2293157458305357\n",
            "\n",
            "Epoch:    1/1     Loss: 2.3329461932182314\n",
            "\n",
            "Epoch:    1/1     Loss: 2.3591766476631166\n",
            "\n",
            "Epoch:    1/1     Loss: 2.1623573422431948\n",
            "\n",
            "Epoch:    1/1     Loss: 2.1656565070152283\n",
            "\n",
            "Epoch:    1/1     Loss: 2.2359654545783996\n",
            "\n",
            "Epoch:    1/1     Loss: 2.469425010681152\n",
            "\n",
            "Epoch:    1/1     Loss: 2.2524495244026186\n",
            "\n",
            "Epoch:    1/1     Loss: 2.119962751865387\n",
            "\n",
            "Epoch:    1/1     Loss: 2.2140596508979797\n",
            "\n",
            "Epoch:    1/1     Loss: 2.1460086345672607\n",
            "\n",
            "Epoch:    1/1     Loss: 2.1151007652282714\n",
            "\n",
            "Epoch:    1/1     Loss: 2.1839763164520263\n",
            "\n",
            "Epoch:    1/1     Loss: 2.2310147643089295\n",
            "\n",
            "Epoch:    1/1     Loss: 2.163479483127594\n",
            "\n",
            "Epoch:    1/1     Loss: 2.1610509514808656\n",
            "\n",
            "Epoch:    1/1     Loss: 2.120169389247894\n",
            "\n",
            "Epoch:    1/1     Loss: 2.256789529323578\n",
            "\n",
            "Epoch:    1/1     Loss: 2.3114621520042418\n",
            "\n",
            "Epoch:    1/1     Loss: 2.297524857521057\n",
            "\n",
            "Epoch:    1/1     Loss: 2.0270156860351562\n",
            "\n",
            "Epoch:    1/1     Loss: 2.0792495727539064\n",
            "\n",
            "Epoch:    1/1     Loss: 2.284024965763092\n",
            "\n",
            "Epoch:    1/1     Loss: 2.2894989132881163\n",
            "\n",
            "Epoch:    1/1     Loss: 2.1355985164642335\n",
            "\n",
            "Epoch:    1/1     Loss: 2.3114584922790526\n",
            "\n",
            "Epoch:    1/1     Loss: 2.2180028796195983\n",
            "\n",
            "Epoch:    1/1     Loss: 2.3206019043922423\n",
            "\n",
            "Epoch:    1/1     Loss: 2.1528802633285524\n",
            "\n",
            "Epoch:    1/1     Loss: 2.173806345462799\n",
            "\n",
            "Epoch:    1/1     Loss: 2.1821242809295653\n",
            "\n",
            "Epoch:    1/1     Loss: 2.127138090133667\n",
            "\n",
            "Epoch:    1/1     Loss: 2.124679183959961\n",
            "\n",
            "Epoch:    1/1     Loss: 2.1276265382766724\n",
            "\n",
            "Epoch:    1/1     Loss: 2.0422821760177614\n",
            "\n",
            "Epoch:    1/1     Loss: 2.162543976306915\n",
            "\n",
            "Epoch:    1/1     Loss: 2.13554847240448\n",
            "\n",
            "Epoch:    1/1     Loss: 2.138112151622772\n",
            "\n",
            "Epoch:    1/1     Loss: 2.1305729746818542\n",
            "\n",
            "Epoch:    1/1     Loss: 2.2086215734481813\n",
            "\n",
            "Epoch:    1/1     Loss: 2.2506447792053224\n",
            "\n",
            "Epoch:    1/1     Loss: 2.156765365600586\n",
            "\n",
            "Epoch:    1/1     Loss: 2.2047871589660644\n",
            "\n",
            "Epoch:    1/1     Loss: 2.1706466913223266\n",
            "\n",
            "Epoch:    1/1     Loss: 2.091162610054016\n",
            "\n",
            "Epoch:    1/1     Loss: 2.053999292850494\n",
            "\n",
            "Epoch:    1/1     Loss: 2.14256808757782\n",
            "\n",
            "Epoch:    1/1     Loss: 2.2629736423492433\n",
            "\n",
            "Epoch:    1/1     Loss: 2.0876692652702333\n",
            "\n",
            "Epoch:    1/1     Loss: 2.135530412197113\n",
            "\n",
            "Epoch:    1/1     Loss: 2.1763742208480834\n",
            "\n",
            "Epoch:    1/1     Loss: 2.1236361861228943\n",
            "\n",
            "Epoch:    1/1     Loss: 2.122711443901062\n",
            "\n",
            "Epoch:    1/1     Loss: 2.1002909183502196\n",
            "\n",
            "Epoch:    1/1     Loss: 2.0748106360435488\n",
            "\n",
            "Epoch:    1/1     Loss: 2.1666842222213747\n",
            "\n",
            "Epoch:    1/1     Loss: 2.162229371070862\n",
            "\n",
            "Epoch:    1/1     Loss: 2.215886378288269\n",
            "\n",
            "Epoch:    1/1     Loss: 2.1099531412124635\n",
            "\n",
            "Epoch:    1/1     Loss: 2.2411527752876284\n",
            "\n",
            "Epoch:    1/1     Loss: 2.21201628446579\n",
            "\n",
            "Epoch:    1/1     Loss: 2.065994691848755\n",
            "\n",
            "Epoch:    1/1     Loss: 2.110330891609192\n",
            "\n",
            "Epoch:    1/1     Loss: 2.0774362444877625\n",
            "\n",
            "Epoch:    1/1     Loss: 2.1273537039756776\n",
            "\n",
            "Epoch:    1/1     Loss: 2.098344600200653\n",
            "\n",
            "Epoch:    1/1     Loss: 2.1652166128158568\n",
            "\n",
            "Epoch:    1/1     Loss: 2.069289970397949\n",
            "\n",
            "Epoch:    1/1     Loss: 2.1125925898551943\n",
            "\n",
            "Epoch:    1/1     Loss: 2.2227993488311766\n",
            "\n",
            "Epoch:    1/1     Loss: 2.0949703574180605\n",
            "\n",
            "Epoch:    1/1     Loss: 2.138960373401642\n",
            "\n",
            "Epoch:    1/1     Loss: 1.974873995780945\n",
            "\n",
            "Epoch:    1/1     Loss: 2.120329713821411\n",
            "\n",
            "Epoch:    1/1     Loss: 2.140919530391693\n",
            "\n",
            "Epoch:    1/1     Loss: 2.1308248162269594\n",
            "\n",
            "Epoch:    1/1     Loss: 2.119924342632294\n",
            "\n",
            "Epoch:    1/1     Loss: 2.038104784488678\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9709662079811097\n",
            "\n",
            "Epoch:    1/1     Loss: 2.019041967391968\n",
            "\n",
            "Epoch:    1/1     Loss: 2.1122297763824465\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9192872405052186\n",
            "\n",
            "Epoch:    1/1     Loss: 2.156795251369476\n",
            "\n",
            "Epoch:    1/1     Loss: 2.112384521961212\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9668734550476075\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9419528126716614\n",
            "\n",
            "Epoch:    1/1     Loss: 2.00254487991333\n",
            "\n",
            "Epoch:    1/1     Loss: 2.0337474584579467\n",
            "\n",
            "Epoch:    1/1     Loss: 2.2533941507339477\n",
            "\n",
            "Epoch:    1/1     Loss: 2.127978801727295\n",
            "\n",
            "Epoch:    1/1     Loss: 2.075601506233215\n",
            "\n",
            "Epoch:    1/1     Loss: 2.138563024997711\n",
            "\n",
            "Epoch:    1/1     Loss: 2.0588520526885987\n",
            "\n",
            "Epoch:    1/1     Loss: 1.986436903476715\n",
            "\n",
            "Epoch:    1/1     Loss: 2.229795718193054\n",
            "\n",
            "Epoch:    1/1     Loss: 2.0521987795829775\n",
            "\n",
            "Epoch:    1/1     Loss: 2.0736628770828247\n",
            "\n",
            "Epoch:    1/1     Loss: 2.1783469557762145\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9482749700546265\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9103485822677613\n",
            "\n",
            "Epoch:    1/1     Loss: 1.910642921924591\n",
            "\n",
            "Epoch:    1/1     Loss: 2.1526618003845215\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9905292868614197\n",
            "\n",
            "Epoch:    1/1     Loss: 2.127096676826477\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9289804100990295\n",
            "\n",
            "Epoch:    1/1     Loss: 2.0259116291999817\n",
            "\n",
            "Epoch:    1/1     Loss: 2.076885199546814\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9148736000061035\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9520413517951964\n",
            "\n",
            "Epoch:    1/1     Loss: 2.0471296668052674\n",
            "\n",
            "Epoch:    1/1     Loss: 2.0857703924179076\n",
            "\n",
            "Epoch:    1/1     Loss: 2.0137531042098997\n",
            "\n",
            "Epoch:    1/1     Loss: 2.0747379660606384\n",
            "\n",
            "Epoch:    1/1     Loss: 2.071502661705017\n",
            "\n",
            "Epoch:    1/1     Loss: 2.1863996505737306\n",
            "\n",
            "Epoch:    1/1     Loss: 2.0937555193901063\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9887145519256593\n",
            "\n",
            "Epoch:    1/1     Loss: 2.0380435585975647\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9533398628234864\n",
            "\n",
            "Epoch:    1/1     Loss: 2.1163246631622314\n",
            "\n",
            "Epoch:    1/1     Loss: 2.0287094473838807\n",
            "\n",
            "Epoch:    1/1     Loss: 2.1465014696121214\n",
            "\n",
            "Epoch:    1/1     Loss: 2.009845495223999\n",
            "\n",
            "Epoch:    1/1     Loss: 2.039668416976929\n",
            "\n",
            "Epoch:    1/1     Loss: 2.133888471126556\n",
            "\n",
            "Epoch:    1/1     Loss: 2.0460974097251894\n",
            "\n",
            "Epoch:    1/1     Loss: 2.047095274925232\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9808661580085754\n",
            "\n",
            "Epoch:    1/1     Loss: 2.0801648139953612\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9777847528457642\n",
            "\n",
            "Epoch:    1/1     Loss: 2.0869196891784667\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9815720200538636\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9674399971961976\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9863263964653015\n",
            "\n",
            "Epoch:    1/1     Loss: 2.1025901198387147\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9515323996543885\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9850232124328613\n",
            "\n",
            "Epoch:    1/1     Loss: 2.108045244216919\n",
            "\n",
            "Epoch:    1/1     Loss: 1.980994999408722\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9864043116569519\n",
            "\n",
            "Epoch:    1/1     Loss: 1.853762400150299\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9968955874443055\n",
            "\n",
            "Epoch:    1/1     Loss: 2.0102161049842833\n",
            "\n",
            "Epoch:    1/1     Loss: 2.0612281918525697\n",
            "\n",
            "Epoch:    1/1     Loss: 1.918989932537079\n",
            "\n",
            "Epoch:    1/1     Loss: 2.0777111291885375\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9915550470352172\n",
            "\n",
            "Epoch:    1/1     Loss: 1.900400495529175\n",
            "\n",
            "Epoch:    1/1     Loss: 1.969648540019989\n",
            "\n",
            "Epoch:    1/1     Loss: 2.152793264389038\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9672257900238037\n",
            "\n",
            "Epoch:    1/1     Loss: 2.0028831243515013\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9756231427192688\n",
            "\n",
            "Epoch:    1/1     Loss: 2.0575992941856383\n",
            "\n",
            "Epoch:    1/1     Loss: 1.980757439136505\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8677055597305299\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9575246095657348\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9222835183143616\n",
            "\n",
            "Epoch:    1/1     Loss: 2.0821842312812806\n",
            "\n",
            "Epoch:    1/1     Loss: 1.95246479511261\n",
            "\n",
            "Epoch:    1/1     Loss: 2.0323369026184084\n",
            "\n",
            "Epoch:    1/1     Loss: 2.01270534992218\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9719375252723694\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9588509440422057\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9857841610908509\n",
            "\n",
            "Epoch:    1/1     Loss: 2.2149588823318482\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9770767331123351\n",
            "\n",
            "Epoch:    1/1     Loss: 2.044814932346344\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9368133902549745\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9993123292922974\n",
            "\n",
            "Epoch:    1/1     Loss: 2.001785600185394\n",
            "\n",
            "Epoch:    1/1     Loss: 1.854221272468567\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9141480207443238\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8881345510482788\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7368916273117065\n",
            "\n",
            "Epoch:    1/1     Loss: 2.0999229192733764\n",
            "\n",
            "Epoch:    1/1     Loss: 2.0172825336456297\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8825713872909546\n",
            "\n",
            "Epoch:    1/1     Loss: 2.1024924516677856\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9509312748908996\n",
            "\n",
            "Epoch:    1/1     Loss: 1.98051974773407\n",
            "\n",
            "Epoch:    1/1     Loss: 2.019594097137451\n",
            "\n",
            "Epoch:    1/1     Loss: 2.044738781452179\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9713699579238892\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9444724678993226\n",
            "\n",
            "Epoch:    1/1     Loss: 2.0064452052116395\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9543280839920043\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9964825510978699\n",
            "\n",
            "Epoch:    1/1     Loss: 1.888155174255371\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9033769726753236\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9122143864631653\n",
            "\n",
            "Epoch:    1/1     Loss: 2.1159361958503724\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9291187047958374\n",
            "\n",
            "Epoch:    1/1     Loss: 1.944287323951721\n",
            "\n",
            "Epoch:    1/1     Loss: 1.949890911579132\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9349684953689574\n",
            "\n",
            "Epoch:    1/1     Loss: 2.1505504131317137\n",
            "\n",
            "Epoch:    1/1     Loss: 1.753538155555725\n",
            "\n",
            "Epoch:    1/1     Loss: 1.994433617591858\n",
            "\n",
            "Epoch:    1/1     Loss: 2.0033377408981323\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9475905537605285\n",
            "\n",
            "Epoch:    1/1     Loss: 2.0055685877799987\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9920511364936828\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8767438888549806\n",
            "\n",
            "Epoch:    1/1     Loss: 2.0376492619514464\n",
            "\n",
            "Epoch:    1/1     Loss: 2.0063549876213074\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9961503744125366\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9537335753440856\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8019502878189086\n",
            "\n",
            "Epoch:    1/1     Loss: 2.0934846878051756\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9491511940956117\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9449820637702941\n",
            "\n",
            "Epoch:    1/1     Loss: 1.932576334476471\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8699324965476989\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9775997877120972\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9890344738960266\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8863858342170716\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9549933910369872\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9328184604644776\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9605460286140441\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9580956816673278\n",
            "\n",
            "Epoch:    1/1     Loss: 2.022257661819458\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9299104332923889\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7009326815605164\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7854714274406434\n",
            "\n",
            "Epoch:    1/1     Loss: 2.0545451641082764\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9934596180915833\n",
            "\n",
            "Epoch:    1/1     Loss: 1.945670485496521\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8124854683876037\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8902822732925415\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8711552023887634\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9479217290878297\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9155073642730713\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8997068285942078\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8553852796554566\n",
            "\n",
            "Epoch:    1/1     Loss: 1.909346067905426\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8411852598190308\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9952668070793151\n",
            "\n",
            "Epoch:    1/1     Loss: 1.787466287612915\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9969345688819886\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8811628222465515\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8983007550239563\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9320310354232788\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8869723796844482\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9555211067199707\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8605680227279664\n",
            "\n",
            "Epoch:    1/1     Loss: 1.822519338130951\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9406265377998353\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8471975326538086\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8903204679489136\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8129154205322267\n",
            "\n",
            "Epoch:    1/1     Loss: 1.999396562576294\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8740721583366393\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9691801905632018\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9314879298210144\n",
            "\n",
            "Epoch:    1/1     Loss: 1.870634710788727\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7336464643478393\n",
            "\n",
            "Epoch:    1/1     Loss: 1.849485945701599\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8335566997528077\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8805203795433045\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8990445375442504\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9964689493179322\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8814857244491576\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9081998705863952\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8133041381835937\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9233328342437743\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8423810124397277\n",
            "\n",
            "Epoch:    1/1     Loss: 1.973626446723938\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9809925198554992\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9355709314346314\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8154128789901733\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8301560163497925\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8805493116378784\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8195587396621704\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8936200976371764\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7980224370956421\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9233336925506592\n",
            "\n",
            "Epoch:    1/1     Loss: 2.002810037136078\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7089978218078614\n",
            "\n",
            "Epoch:    1/1     Loss: 2.0734606862068174\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9021312236785888\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9173779606819152\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8387850999832154\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8919942378997803\n",
            "\n",
            "Epoch:    1/1     Loss: 1.942652440071106\n",
            "\n",
            "Epoch:    1/1     Loss: 2.0895390272140504\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9012848734855652\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8218454241752624\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8752028465270996\n",
            "\n",
            "Epoch:    1/1     Loss: 1.863933253288269\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9049994230270386\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9105766534805297\n",
            "\n",
            "Epoch:    1/1     Loss: 1.884067177772522\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8821284770965576\n",
            "\n",
            "Epoch:    1/1     Loss: 1.817030417919159\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8609593868255616\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8406435251235962\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9955295324325562\n",
            "\n",
            "Epoch:    1/1     Loss: 1.826030933856964\n",
            "\n",
            "Epoch:    1/1     Loss: 2.0074951171875\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8488515377044679\n",
            "\n",
            "Epoch:    1/1     Loss: 1.81920382976532\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8782567620277404\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9193498611450195\n",
            "\n",
            "Epoch:    1/1     Loss: 1.891092562675476\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8575086712837219\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8569436550140381\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8326255917549132\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9502296686172484\n",
            "\n",
            "Epoch:    1/1     Loss: 1.835215401649475\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8241493582725525\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8492570757865905\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7867610096931457\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8385034441947936\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8259156942367554\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8495301723480224\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8462576270103455\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9588377952575684\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7517563343048095\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8824007630348205\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7622858047485352\n",
            "\n",
            "Epoch:    1/1     Loss: 1.766936755180359\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7721968650817872\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9108983993530273\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8844515562057496\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7619143724441528\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8911985635757447\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8701650142669677\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8650861144065858\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8626213550567627\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8653589606285095\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7993121147155762\n",
            "\n",
            "Epoch:    1/1     Loss: 1.874507451057434\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8554029703140258\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8735288381576538\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9615796208381653\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7061894416809082\n",
            "\n",
            "Epoch:    1/1     Loss: 1.903434193134308\n",
            "\n",
            "Epoch:    1/1     Loss: 1.76480233669281\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9594366312026978\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7954347848892211\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9454047918319701\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9895521402359009\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7723785281181335\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8869228124618531\n",
            "\n",
            "Epoch:    1/1     Loss: 1.857563555240631\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8046915411949158\n",
            "\n",
            "Epoch:    1/1     Loss: 1.929232382774353\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9407655835151671\n",
            "\n",
            "Epoch:    1/1     Loss: 1.875977599620819\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7041378736495971\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9866806507110595\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7748616099357606\n",
            "\n",
            "Epoch:    1/1     Loss: 1.826017963886261\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8396844387054443\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8860090970993042\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7951190710067748\n",
            "\n",
            "Epoch:    1/1     Loss: 1.881699538230896\n",
            "\n",
            "Epoch:    1/1     Loss: 1.733760404586792\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8033921122550964\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7079177141189574\n",
            "\n",
            "Epoch:    1/1     Loss: 1.765666377544403\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8988190412521362\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7528910756111145\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7607975959777833\n",
            "\n",
            "Epoch:    1/1     Loss: 1.762317144870758\n",
            "\n",
            "Epoch:    1/1     Loss: 1.838506245613098\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8827139735221863\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8168530106544494\n",
            "\n",
            "Epoch:    1/1     Loss: 1.67730393409729\n",
            "\n",
            "Epoch:    1/1     Loss: 1.847187626361847\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8999265074729919\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7461713910102845\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8355252861976623\n",
            "\n",
            "Epoch:    1/1     Loss: 1.741932713985443\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7330224514007568\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8527210354804993\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7132703185081481\n",
            "\n",
            "Epoch:    1/1     Loss: 1.869281780719757\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7602913498878479\n",
            "\n",
            "Epoch:    1/1     Loss: 1.815038013458252\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7509855389595033\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7240394353866577\n",
            "\n",
            "Epoch:    1/1     Loss: 1.6210522413253785\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8686357140541077\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8310790181159973\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9632368087768555\n",
            "\n",
            "Epoch:    1/1     Loss: 1.775398886203766\n",
            "\n",
            "Epoch:    1/1     Loss: 1.870349156856537\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7836564183235168\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7726448893547058\n",
            "\n",
            "Epoch:    1/1     Loss: 1.6771417737007142\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7799354791641235\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7631215929985047\n",
            "\n",
            "Epoch:    1/1     Loss: 1.808189594745636\n",
            "\n",
            "Epoch:    1/1     Loss: 1.802665913105011\n",
            "\n",
            "Epoch:    1/1     Loss: 1.625382125377655\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8281849145889282\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7255398035049438\n",
            "\n",
            "Epoch:    1/1     Loss: 1.6687439203262329\n",
            "\n",
            "Epoch:    1/1     Loss: 1.808045244216919\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9327704906463623\n",
            "\n",
            "Epoch:    1/1     Loss: 1.724394476413727\n",
            "\n",
            "Epoch:    1/1     Loss: 1.913526678085327\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7522619485855102\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7431085705757141\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9670873165130616\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7547746062278748\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7932572841644288\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7721626877784729\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7349462270736695\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8938873410224915\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8715901136398316\n",
            "\n",
            "Epoch:    1/1     Loss: 1.941787874698639\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7251739740371703\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8584457159042358\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7228423714637757\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7981773495674134\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8410171627998353\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7538073658943176\n",
            "\n",
            "Epoch:    1/1     Loss: 1.846091389656067\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8200758337974547\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7697703957557678\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8705556988716125\n",
            "\n",
            "Epoch:    1/1     Loss: 1.866479742527008\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8834880471229554\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8394646048545837\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7253337144851684\n",
            "\n",
            "Epoch:    1/1     Loss: 1.6569450497627258\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8840332746505737\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7576650857925415\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7871586799621582\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7018312215805054\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8525386929512024\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8506844520568848\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7602121829986572\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9311591625213622\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7358185529708863\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8180914640426635\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8503240466117858\n",
            "\n",
            "Epoch:    1/1     Loss: 1.709938359260559\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7917260885238648\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7775705575942993\n",
            "\n",
            "Epoch:    1/1     Loss: 1.998131799697876\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8356719374656678\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8204681158065796\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8111325740814208\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8328190684318542\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7610782742500306\n",
            "\n",
            "Epoch:    1/1     Loss: 1.766901397705078\n",
            "\n",
            "Epoch:    1/1     Loss: 1.835995876789093\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7240944504737854\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8274389624595642\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7433129668235778\n",
            "\n",
            "Epoch:    1/1     Loss: 1.66421377658844\n",
            "\n",
            "Epoch:    1/1     Loss: 1.6996032357215882\n",
            "\n",
            "Epoch:    1/1     Loss: 1.884954583644867\n",
            "\n",
            "Epoch:    1/1     Loss: 1.688280165195465\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8633733153343202\n",
            "\n",
            "Epoch:    1/1     Loss: 1.702983522415161\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7377966403961183\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7109305739402771\n",
            "\n",
            "Epoch:    1/1     Loss: 1.750411856174469\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7364723682403564\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8690945982933045\n",
            "\n",
            "Epoch:    1/1     Loss: 1.822542178630829\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8997807621955871\n",
            "\n",
            "Epoch:    1/1     Loss: 1.6872408747673036\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7039652228355409\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7631285309791564\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7577293038368225\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8036054372787476\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8139240145683289\n",
            "\n",
            "Epoch:    1/1     Loss: 1.802346158027649\n",
            "\n",
            "Epoch:    1/1     Loss: 1.8183408737182618\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7688957095146178\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7416698932647705\n",
            "\n",
            "Epoch:    1/1     Loss: 1.9016730070114136\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7357155680656433\n",
            "\n",
            "Epoch:    1/1     Loss: 1.7575678825378418\n",
            "\n",
            "Epoch:    1/1     Loss: 1.783808135986328\n",
            "\n",
            "Model Trained and Saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dJenBJipCmQH",
        "outputId": "61bd0fa2-8bc1-4edd-d0af-f649e72b5515",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "torch.__version__"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.2.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Fwjmmn0pyF0M",
        "outputId": "720ae597-251d-44e4-af52-eabd38f76869",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import torch\n",
        "import helper\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def generate(rnn, prime_id, int_to_vocab, predict_len=100):\n",
        "    \"\"\"\n",
        "    Generate text using the neural network\n",
        "    :param decoder: The PyTorch Module that holds the trained neural network\n",
        "    :param prime_id: The word id to start the first prediction\n",
        "    :param int_to_vocab: Dict of word id keys to word values\n",
        "    :param token_dict: Dict of puncuation tokens keys to puncuation values\n",
        "    :param pad_value: The value used to pad a sequence\n",
        "    :param predict_len: The length of text to generate\n",
        "    :return: The generated text\n",
        "    \"\"\"\n",
        "    rnn.eval()\n",
        "    \n",
        "    # create a sequence (batch_size=1) with the prime_id\n",
        "    pad_value = 0 # 这个设置成0好些, 其他好像都有点坑\n",
        "    current_seq = np.full((1, sequence_length), pad_value)\n",
        "    current_seq[-1][-len(prime_id):] = np.array(prime_id)\n",
        "    predicted = [int_to_vocab[each] for each in prime_id]\n",
        "    \n",
        "    for _ in range(predict_len):\n",
        "        if train_on_gpu:\n",
        "            current_seq = torch.LongTensor(current_seq).cuda()\n",
        "        else:\n",
        "          current_seq = torch.LongTensor(current_seq)\n",
        "        \n",
        "        # initialize the hidden state\n",
        "        hidden = rnn.init_hidden(current_seq.size(0))\n",
        "        \n",
        "        # get the output of the rnn\n",
        "        output, _ = rnn(current_seq, hidden)\n",
        "        \n",
        "        # get the next word probabilities\n",
        "        p = F.softmax(output, dim=1).data\n",
        "        if(train_on_gpu):\n",
        "            p = p.cpu() # move to cpu\n",
        "         \n",
        "        # use top_k sampling to get the index of the next word\n",
        "        top_k = 5\n",
        "        p, top_i = p.topk(top_k)\n",
        "        top_i = top_i.numpy().squeeze()\n",
        "        \n",
        "        # select the likely next word index with some element of randomness\n",
        "        p = p.numpy().squeeze()\n",
        "        word_i = np.random.choice(top_i, p=p/p.sum())\n",
        "        \n",
        "        # retrieve that word from the dictionary\n",
        "        word = int_to_vocab[word_i]\n",
        "        predicted.append(word)     \n",
        "        \n",
        "        # the generated word becomes the next \"current sequence\" and the cycle can continue\n",
        "        current_seq = np.roll(current_seq.cpu(), -1, 1)\n",
        "        current_seq[-1][-1] = word_i\n",
        "    \n",
        "    gen_sentences = ' '.join(predicted)\n",
        "    \n",
        "    gen_sentences = gen_sentences.replace('\\n ', '\\n')\n",
        "    gen_sentences = gen_sentences.replace('( ', '(')\n",
        "    \n",
        "    # return all the sentences\n",
        "    return gen_sentences\n",
        "\n",
        "gen_length = 400 # modify the length to your preference\n",
        "prime_word = ['#', 'include', '<', 'iostream'] # name for starting the script\n",
        "\n",
        "trained_rnn = helper.load_model('./trained_rnn_1')\n",
        "generated_script = generate(trained_rnn, [char2int[each] for each in prime_word], int2char, gen_length)\n",
        "print(generated_script)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# include < iostream > # include < vector > using namespace std ; int main () { int a [ 1001 ] , n , i = 0 ; for (register int i = 0 ; i < n ; + + i ) { cout < < n [ i ] ; cout < < endl ; } int main () { int n , m , m ; cin > > n ; for (int i = 1 ; i < = n ; + + i ) { cin > > a > > b ; for (int i = 1 ; i < = 4 ; i + + ) { if (! vis [ v ] [ i ] & & s [ j + + ] = = 0 ) return false ; } if ((i + 1 ) % 2 = 1 ; if (n < = m ) { if (i = = 0 ) { tmp + = intToStr (f _ m , i , x , y , y ) ; } } } void loop (int x ) { for (int i = 1 ; i < = 10 ; i + + ) { for (int j = 1 ; j < = n ; + + l ) { / / } cout < < \" No \\ n \\ n \" ; / / cout < < \" - - - - - - - - - - - - - - - - - - - - - - \" < < endl ; cout < < endl ; } int main () { int n ; while (scanf (\" % d \" , & a [ i ] ) ; for (int j = 0 ; j < 3 ; j + + ) { / / cout < < endl ; } / * 3 . 0 , 0 , 0 . 0 , 0 , 0 ) ; for (int i = 1 ; i < = m ; + + i ) { if (str [ i ] [ j ] = ' 0 ' ; } else { if (str [ i ] = = ' = ' ) return true ;\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "k1JGA_uLCmQK"
      },
      "source": [
        "### 问题: 你如何决定你的模型超参数？\n",
        "比如，你是否试过不同的 different sequence_lengths 并发现哪个使得模型的收敛速度变化？那你的隐藏层数和层数呢？你是如何决定使用这个网络参数的？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "APDKA3s0CmQK"
      },
      "source": [
        "**答案:** \n",
        "\n",
        "我调整了各种参数, 比如sequence_lengths, 刚开始设置成100, 但发现效果不是很好. 后来又设置成20, 效果较好. \n",
        "\n",
        "我的hidden_dim设置成了512, embedding_dim设置成了256, n_layers设置为2, 这几个参数参数越大, 意味着模型越复杂, 在经过很多次调整和测试之后, 我发现这几个参数的设置是相对来说更合理的.\n",
        "\n",
        "我训练了7个epoch, 最终的loss值为3.25左右.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fqCI1J1nCmQL"
      },
      "source": [
        "*italicized text*---\n",
        "# 检查点\n",
        "\n",
        "通过运行上面的训练单元，你的模型已经以`trained_rnn`名字存储，如果你存储了你的notebook， **你可以在之后的任何时间来访问你的代码和结果**. 下述代码可以帮助你重载你的结果!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rQYOmSvvCmQM",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "DON'T MODIFY ANYTHING IN THIS CELL\n",
        "\"\"\"\n",
        "import torch\n",
        "import helper\n",
        "import problem_unittests as tests\n",
        "\n",
        "_, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n",
        "trained_rnn = helper.load_model('./save/trained_rnn')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1GeftylcCmQR"
      },
      "source": [
        "## 生成电视剧剧本\n",
        "你现在可以生成你的“假”电视剧剧本啦！\n",
        "\n",
        "### 生成文字\n",
        "你的神经网络会不断重复生成一个单词，直到生成满足你要求长度的剧本。使用 `generate` 函数来完成上述操作。首先，使用 `prime_id` 来生成word id，之后确定生成文本长度 `predict_len`。同时， topk 采样来引入文字选择的随机性!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t5HAc_HzCmQR",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
        "\"\"\"\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def generate(rnn, prime_id, int_to_vocab, token_dict, pad_value, predict_len=100):\n",
        "    \"\"\"\n",
        "    Generate text using the neural network\n",
        "    :param decoder: The PyTorch Module that holds the trained neural network\n",
        "    :param prime_id: The word id to start the first prediction\n",
        "    :param int_to_vocab: Dict of word id keys to word values\n",
        "    :param token_dict: Dict of puncuation tokens keys to puncuation values\n",
        "    :param pad_value: The value used to pad a sequence\n",
        "    :param predict_len: The length of text to generate\n",
        "    :return: The generated text\n",
        "    \"\"\"\n",
        "    rnn.eval()\n",
        "    \n",
        "    # create a sequence (batch_size=1) with the prime_id\n",
        "    current_seq = np.full((1, sequence_length), pad_value)\n",
        "    current_seq[-1][-1] = prime_id\n",
        "    predicted = [int_to_vocab[prime_id]]\n",
        "    \n",
        "    for _ in range(predict_len):\n",
        "        if train_on_gpu:\n",
        "            current_seq = torch.LongTensor(current_seq).cuda()\n",
        "        else:\n",
        "          current_seq = torch.LongTensor(current_seq)\n",
        "        \n",
        "        # initialize the hidden state\n",
        "        hidden = rnn.init_hidden(current_seq.size(0))\n",
        "        \n",
        "        # get the output of the rnn\n",
        "        output, _ = rnn(current_seq, hidden)\n",
        "        \n",
        "        # get the next word probabilities\n",
        "        p = F.softmax(output, dim=1).data\n",
        "        if(train_on_gpu):\n",
        "            p = p.cpu() # move to cpu\n",
        "         \n",
        "        # use top_k sampling to get the index of the next word\n",
        "        top_k = 5\n",
        "        p, top_i = p.topk(top_k)\n",
        "        top_i = top_i.numpy().squeeze()\n",
        "        \n",
        "        # select the likely next word index with some element of randomness\n",
        "        p = p.numpy().squeeze()\n",
        "        word_i = np.random.choice(top_i, p=p/p.sum())\n",
        "        \n",
        "        # retrieve that word from the dictionary\n",
        "        word = int_to_vocab[word_i]\n",
        "        predicted.append(word)     \n",
        "        \n",
        "        # the generated word becomes the next \"current sequence\" and the cycle can continue\n",
        "        current_seq = np.roll(current_seq.cpu(), -1, 1)\n",
        "        current_seq[-1][-1] = word_i\n",
        "    \n",
        "    gen_sentences = ' '.join(predicted)\n",
        "    \n",
        "    # Replace punctuation tokens\n",
        "    for key, token in token_dict.items():\n",
        "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
        "        gen_sentences = gen_sentences.replace(' ' + token.lower(), key)\n",
        "    gen_sentences = gen_sentences.replace('\\n ', '\\n')\n",
        "    gen_sentences = gen_sentences.replace('( ', '(')\n",
        "    \n",
        "    # return all the sentences\n",
        "    return gen_sentences\n",
        "\n",
        "gen_length = 500 # modify the length to your preference\n",
        "prime_word = 'int' # name for starting the script\n",
        "\n",
        "\"\"\"\n",
        "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
        "\"\"\"\n",
        "pad_word = helper.SPECIAL_WORDS['PADDING']\n",
        "generated_script = generate(trained_rnn, vocab_to_int[prime_word], int_to_vocab, token_dict, vocab_to_int[pad_word], gen_length)\n",
        "print(generated_script)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YRryUfDHCmQV"
      },
      "source": [
        "### 生成一个新剧本\n",
        "是时候生成一个剧本啦。设置`gen_length` 剧本长度，设置 `prime_word`为以下任意词来开始生成吧:\n",
        "- \"jerry\"\n",
        "- \"elaine\"\n",
        "- \"george\"\n",
        "- \"kramer\"\n",
        "\n",
        "你可以把prime word 设置成 _任意 _ 单词, 但是使用名字开始会比较好(任何其他名字也是可以哒!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F8ACQCJ6CmQY",
        "outputId": "0584af75-e062-4f93-fd26-99da7a1db5ee",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# run the cell multiple times to get different results!\n",
        "gen_length = 500 # modify the length to your preference\n",
        "prime_word = 'int' # name for starting the script\n",
        "\n",
        "\"\"\"\n",
        "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
        "\"\"\"\n",
        "pad_word = helper.SPECIAL_WORDS['PADDING']\n",
        "generated_script = generate(trained_rnn, vocab_to_int[prime_word], int_to_vocab, token_dict, vocab_to_int[pad_word], gen_length)\n",
        "print(generated_script)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jerry:.\n",
            "\n",
            "elaine: what is this? i thought you liked her?\n",
            "\n",
            "george: no.\n",
            "\n",
            "jerry:(smiling) i don't want to get out of here.\n",
            "\n",
            "george:(smiling, not) oh, no!\n",
            "\n",
            "george: i don't know what to do, you know, it's a good idea.\n",
            "\n",
            "jerry: you don't know how you feel?\n",
            "\n",
            "elaine:(quietly).(to kramer)\n",
            "\n",
            "kramer:(quietly)\n",
            "\n",
            "jerry:(on the phone) yeah, that's what they do.\n",
            "\n",
            "kramer: oh, yeah.\n",
            "\n",
            "elaine: you got that?\n",
            "\n",
            "jerry:(looking up to the van) hey, you got it?\n",
            "\n",
            "jerry:(to the manager of the conversation) oh, you don't have to do that.\n",
            "\n",
            "elaine: you have to go to the bathroom.(george shakes his head)\n",
            "\n",
            "elaine: oh! i think you should see that.\n",
            "\n",
            "george:(to the waitress) yeah.\n",
            "\n",
            "jerry:(to kramer) hey, you know what? i was just trying to get away, i know what i'm saying.\n",
            "\n",
            "kramer: i don't have to...\n",
            "\n",
            "george:(quietly) oh, you got it.\n",
            "\n",
            "jerry: oh, hi.\n",
            "\n",
            "jerry: hi.\n",
            "\n",
            "george:(to jerry) well, i don't know.\n",
            "\n",
            "jerry:(to george) i know... i know...\n",
            "\n",
            "kramer: well, i don't want to know if i can do something.\n",
            "\n",
            "george: what do you think?\n",
            "\n",
            "jerry: yeah.\n",
            "\n",
            "jerry:(looking over the speaker) you are the same.\n",
            "\n",
            "jerry: i can't go back to the game.\n",
            "\n",
            "elaine: i think you can get it in the car!\n",
            "\n",
            "jerry: oh, no no, i don't know. i'm just trying to get some sleep.\n",
            "\n",
            "jerry: oh, i was just thinking of a woman.\n",
            "\n",
            "jerry: i don't know...\n",
            "\n",
            "george: you know the last one of these guys are going to be a lot of money.(he exits the street to the kitchen)\n",
            "\n",
            "kramer:(pointing at him) hey, what are you doing with this?\n",
            "\n",
            "jerry: what are you gonna tell you?\n",
            "\n",
            "jerry: what?\n",
            "\n",
            "elaine: well, you know, i think i'm sorry.\n",
            "\n",
            "elaine: well, i just came from the airport.\n",
            "\n",
            "george: i know, it's not the point.(she starts to get away)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yXXH_4ynCmQb"
      },
      "source": [
        "#### 存下你最爱的片段\n",
        "\n",
        "一旦你发现一段有趣或者好玩的片段，就把它存下啦！"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F2m0G6XCCmQc",
        "colab": {}
      },
      "source": [
        "# save script to a text file\n",
        "f =  open(\"generated_script_1.txt\",\"w\")\n",
        "f.write(generated_script)\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6-mUPYLXCmQd"
      },
      "source": [
        "# 这个电视剧剧本是无意义的\n",
        "如果你的电视剧剧本不是很有逻辑也是ok的。下面是一个例子。\n",
        "\n",
        "### 生成剧本案例\n",
        "\n",
        ">jerry: what about me?\n",
        ">\n",
        ">jerry: i don't have to wait.\n",
        ">\n",
        ">kramer:(to the sales table)\n",
        ">\n",
        ">elaine:(to jerry) hey, look at this, i'm a good doctor.\n",
        ">\n",
        ">newman:(to elaine) you think i have no idea of this...\n",
        ">\n",
        ">elaine: oh, you better take the phone, and he was a little nervous.\n",
        ">\n",
        ">kramer:(to the phone) hey, hey, jerry, i don't want to be a little bit.(to kramer and jerry) you can't.\n",
        ">\n",
        ">jerry: oh, yeah. i don't even know, i know.\n",
        ">\n",
        ">jerry:(to the phone) oh, i know.\n",
        ">\n",
        ">kramer:(laughing) you know...(to jerry) you don't know.\n",
        "\n",
        "\n",
        "如果这个电视剧剧本毫无意义，那也没有关系。我们的训练文本不到一兆字节。为了获得更好的结果，你需要使用更小的词汇范围或是更多数据。幸运的是，我们的确拥有更多数据！在本项目开始之初我们也曾提过，这是[另一个数据集](https://www.kaggle.com/wcukierski/the-simpsons-by-the-data)的子集。我们并没有让你基于所有数据进行训练，因为这将耗费大量时间。然而，你可以随意使用这些数据训练你的神经网络。当然，是在完成本项目之后。\n",
        "# 提交项目\n",
        "在提交项目时，请确保你在保存 notebook 前运行了所有的单元格代码。请将 notebook 文件保存为 \"dlnd_tv_script_generation.ipynb\"，并将它作为 HTML 文件保存在 \"File\" -> \"Download as\" 中。请将 \"helper.py\" 和 \"problem_unittests.py\" 文件一并打包成 zip 文件提交。"
      ]
    }
  ]
}